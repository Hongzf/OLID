{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Fasttext.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNjZXlKHmAAX1164ayLhvhR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hongzf/OLID/blob/main/ML_Fasttext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bz8M9IjAvJa-"
      },
      "outputs": [],
      "source": [
        "!pip install emoji\n",
        "!pip install wordsegment\n",
        "!pip install transformers\n",
        "!pip install trainer\n",
        "!pip install attention"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import os\n",
        "import emoji\n",
        "from wordsegment import load, segment\n",
        "import pickle\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import os,re\n",
        "from bs4 import BeautifulSoup\n",
        "from gensim import models\n",
        "\n",
        "from transformers import BertTokenizer, RobertaTokenizer, get_cosine_schedule_with_warmup\n",
        "from trainer import Trainer \n",
        "from torch import nn\n",
        "from transformers import BertModel, BertForSequenceClassification, RobertaForSequenceClassification, RobertaModel\n",
        "from attention import Attention\n",
        "load()"
      ],
      "metadata": {
        "id": "SWNxHnwBvQXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#optional Google drive integration\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ifElmEMvQ9U",
        "outputId": "4b0275ee-0dc3-4b96-9f69-1620a6736e1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OLID_PATH = './drive/MyDrive/OLID'\n",
        "SAVE_PATH = '.drive/MyDrive/OLID/save'\n",
        "LABEL_DICT = {\n",
        "    'a': {'OFF': 0, 'NOT': 1},\n",
        "    'b': {'TIN': 0, 'UNT': 1, 'NULL': 2},\n",
        "    'c': {'IND': 0, 'GRP': 1, 'OTH': 2, 'NULL': 3}\n",
        "}\n",
        "\n",
        "TRAIN_PATH = os.path.join(OLID_PATH, 'olid-training-v1.0.tsv')"
      ],
      "metadata": {
        "id": "-Wzk5q0dvTP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = 'c'\n",
        "# model_name = 'roberta'\n",
        "# model_size = 'base'\n",
        "truncate = 512\n",
        "epochs = 10\n",
        "lr = 0.0001\n",
        "wd = 0.0\n",
        "bs = 50 #72\n",
        "patience = 5"
      ],
      "metadata": {
        "id": "wiJSNQoovXDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def emoji2word(sents):\n",
        "    return [emoji.demojize(sent) for sent in sents]\n",
        "\n",
        "def remove_useless_punctuation(sents):\n",
        "    for i, sent in enumerate(sents):\n",
        "        sent = sent.replace(':', ' ')\n",
        "        sent = sent.replace('_', ' ')\n",
        "        sent = sent.replace('...', ' ')\n",
        "        sents[i] = sent\n",
        "    return sents\n",
        "\n",
        "def remove_replicates(sents):\n",
        "    # if there are multiple `@USER` tokens in a tweet, replace it with `@USERS`\n",
        "    # because some tweets contain so many `@USER` which may cause redundant\n",
        "    for i, sent in enumerate(sents):\n",
        "        if sent.find('@USER') != sent.rfind('@USER'):\n",
        "            sents[i] = sent.replace('@USER', '')\n",
        "            sents[i] = '@USERS ' + sents[i]\n",
        "    return sents\n",
        "\n",
        "def replace_rare_words(sents):\n",
        "    rare_words = {\n",
        "        'URL': 'http'\n",
        "    }\n",
        "    for i, sent in enumerate(sents):\n",
        "        for w in rare_words.keys():\n",
        "            sents[i] = sent.replace(w, rare_words[w])\n",
        "    return sents\n",
        "\n",
        "def segment_hashtag(sents):\n",
        "    # E.g. '#LunaticLeft' => 'lunatic left'\n",
        "    for i, sent in enumerate(sents):\n",
        "        sent_tokens = sent.split(' ')\n",
        "        for j, t in enumerate(sent_tokens):\n",
        "            if t.find('#') == 0:\n",
        "                sent_tokens[j] = ' '.join(segment(t))\n",
        "        sents[i] = ' '.join(sent_tokens)\n",
        "    return sents\n",
        "\n",
        "\n",
        "def process_tweets(tweets):\n",
        "    tweets = emoji2word(tweets)\n",
        "    tweets = replace_rare_words(tweets)\n",
        "    tweets = remove_replicates(tweets)\n",
        "    tweets = segment_hashtag(tweets)\n",
        "    tweets = remove_useless_punctuation(tweets)\n",
        "    tweets = np.array(tweets)\n",
        "    return tweets"
      ],
      "metadata": {
        "id": "JYs1e1drvZeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.io.formats.info import DataFrameInfo\n",
        "df = pd.read_csv(TRAIN_PATH, sep='\\t', keep_default_na=False)\n",
        "\n",
        "if task == 'a':\n",
        "        df = df\n",
        "elif task == 'b':\n",
        "        df=df[~df['subtask_b'].isin(['NULL'])]\n",
        "elif task == 'c':\n",
        "        df=df[~df['subtask_c'].isin(['NULL'])]\n",
        "\n",
        "\n",
        "ids = np.array(df['id'].values)\n",
        "tweets = np.array(df['tweet'].values)\n",
        "\n",
        "print(tweets[1])\n",
        "# Process tweets\n",
        "df['tweet'] = process_tweets(tweets)\n",
        "\n",
        "tweets_2 = df['tweet']\n",
        "print(tweets_2[1])\n",
        "\n",
        "label_a = np.array(df['subtask_a'].values)\n",
        "label_b = df['subtask_b'].values\n",
        "label_c = np.array(df['subtask_c'].values)\n",
        "nums = len(df)\n",
        "\n",
        "if task == 'a':\n",
        "        df_train = df.copy()\n",
        "        df_train = df_train.drop(['subtask_b','subtask_c'],axis = 1)\n",
        "elif task == 'b':\n",
        "        df_train = df.copy()\n",
        "        df_train = df_train.drop(['subtask_a','subtask_c'],axis = 1)\n",
        "        df_train=df_train[~df_train['subtask_b'].isin(['NULL'])]\n",
        "elif task == 'c':\n",
        "        df_train = df.copy()\n",
        "        df_train = df_train.drop(['subtask_a','subtask_b'],axis = 1)\n",
        "        df_train=df_train[~df_train['subtask_c'].isin(['NULL'])]\n",
        "\n",
        "\n",
        "\n",
        "# read_test_file(task, tokenizer, truncate=512):\n",
        "df1 = pd.read_csv(os.path.join(OLID_PATH, 'testset-level' + task + '.tsv'), sep='\\t')\n",
        "df2 = pd.read_csv(os.path.join(OLID_PATH, 'labels-level' + task + '.csv'), sep=',')\n",
        "ids = np.array(df1['id'].values)\n",
        "# tweets = np.array(df1['tweet'].values)\n",
        "# labels = np.array(df2['label'].values)\n",
        "# nums = len(df1)\n",
        "df_test = pd.merge(df1,df2,on='id')\n",
        "# Process tweets\n",
        "tweets_test = df_test['tweet']\n",
        "df_test['tweet'] = process_tweets(tweets_test)  "
      ],
      "metadata": {
        "id": "xvMBidfFvdxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = tweets_2\n",
        "x_val = tweets_test\n",
        "Y_train_name = 'subtask_'+ task\n",
        "y_train = df_train[Y_train_name]\n",
        "y_val = df_test['label']"
      ],
      "metadata": {
        "id": "QjDnN-p5vgQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_total = pd.concat([x_train,x_val])\n",
        "print(x_total)"
      ],
      "metadata": {
        "id": "kr_sJr7uf4ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['label'].isnull().any"
      ],
      "metadata": {
        "id": "zcCEJSIEL5SV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.isnull().any"
      ],
      "metadata": {
        "id": "Peaz7jurM7ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_val.isnull().any"
      ],
      "metadata": {
        "id": "R-Ot-0loOhe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_val.isnull().any"
      ],
      "metadata": {
        "id": "1ZrEXsofOmEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "uqlJIRoh_aVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "\n",
        "num_features = 300   \n",
        "min_word_count = 1    \n",
        "num_workers = 4        \n",
        "context = 10          \n",
        "model_ = 0             \n",
        "\n",
        "model_name = '{}features_{}minwords_{}context.model'.format(num_features, min_word_count, context)\n",
        "\n",
        "model_name_2 = 'fasttext.model'\n",
        "\n",
        "print('Training model...')\n",
        "model_2 = models.FastText(x_total, size=num_features, window=context, min_count=min_word_count,\\\n",
        "                        sg = model_, min_n = 3 , max_n = 2, alpha=0.025, min_alpha=0.00025)\n",
        "# , min_n = 2 , max_n = 3\n",
        "#Save\n",
        "# model_2.save(os.path.join('..', 'models', model_name_2))\n",
        "# model_2.wv.save_word2vec_format(os.path.join('..','models','fasttext.txt'),binary = False)\n",
        "\n",
        "if task == 'a':\n",
        "        model_2.save('./drive/MyDrive/OLID/save/ML_embedding/fasttext_model')\n",
        "elif task == 'b':\n",
        "        model_2.save('./drive/MyDrive/OLID/save/ML_embedding/b_fasttext_model')\n",
        "elif task == 'c':\n",
        "        model_2.save('./drive/MyDrive/OLID/save/ML_embedding/c_fasttext_model')\n",
        "\n",
        "model_2.save('./drive/MyDrive/OLID/save/ML_embedding/fasttext_model')\n",
        "# model_2.wv.save_word2vec_format(('./drive/MyDrive/OLID/save/ML_embedding/fasttext.txt'),binary = False)\n"
      ],
      "metadata": {
        "id": "tmuQ0Mw3viRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_fast = 'fasttext.model'\n",
        "\n",
        "if task == 'a':\n",
        "        fasttext_embedding = models.FastText.load(('./drive/MyDrive/OLID/save/ML_embedding/fasttext_model'))\n",
        "elif task == 'b':\n",
        "        fasttext_embedding = models.FastText.load(('./drive/MyDrive/OLID/save/ML_embedding/b_fasttext_model'))\n",
        "elif task == 'c':\n",
        "        fasttext_embedding = models.FastText.load(('./drive/MyDrive/OLID/save/ML_embedding/c_fasttext_model'))\n",
        "        \n",
        "# model_file =('./drive/MyDrive/OLID/save/ML_embedding/fasttext_model')\n",
        "# fasttext_embedding = model.load_fasttext_format（model_file，encoding ='utf8'，full_model = True ）"
      ],
      "metadata": {
        "id": "uE1-0Lokdy0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words=\"english\"\n",
        "def stopwords_clean(text):\n",
        "    text = [w for w in text if w not in stop_words]\n",
        "    return text\n",
        "\n",
        "\n",
        "def to_review_vector(review,model='fasttext'):\n",
        "    words = process_tweets(review)\n",
        "    if model == 'word2vec':\n",
        "        array = np.asarray([word2vec_embedding[w] for w in words if w in word2vec_embedding],dtype='float32')\n",
        "    elif model == 'glove':\n",
        "        array = np.asarray([glove_embedding[w] for w in words if w in glove_embedding],dtype='float32')\n",
        "    elif model == 'fasttext':\n",
        "        array = np.asarray([fasttext_embedding[w] for w in words if w in fasttext_embedding],dtype='float32')\n",
        "    else:\n",
        "        raise ValueError('INPUT：word2vec、glove or fasttext')\n",
        "    return array.mean(axis=0)\n",
        "    "
      ],
      "metadata": {
        "id": "-XW2m1u1vlKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"sample\"\"\"\n",
        "train_data_fasttext = [to_review_vector(text,'fasttext') for text in df['tweet']]"
      ],
      "metadata": {
        "id": "gvAWpn4GvoTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_fasttext = [to_review_vector(text,'fasttext') for text in df_test['tweet']]"
      ],
      "metadata": {
        "id": "Hzc9rnvW89g4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for item in train_data_fasttext:\n",
        "    if np.isnan(item).any():\n",
        "        print('yes')"
      ],
      "metadata": {
        "id": "ndapdHSKNonM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.isnan(train_data_fasttext).any()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBG2_hdhzhQM",
        "outputId": "8c0bb153-d408-498f-b3e7-831b32521002"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.isnan(test_data_fasttext).any()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRhQuj0lNVtB",
        "outputId": "3b57014d-26b0-498c-89c6-384e2f5b552a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear SVM\n"
      ],
      "metadata": {
        "id": "GQ7jDk3Fw1Md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "# load the machine model form sklearn\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import model_selection, naive_bayes, svm\n",
        "\n",
        "# Define the parameters to tune\n",
        "parameter_grid = { \n",
        "    'C': [1, 3],\n",
        "    'gamma': [1, 'auto', 'scale']\n",
        "}\n",
        "# setup the SVM model\n",
        "# SVM_model = GridSearchCV(svm.SVC(kernel='linear'), parameter_grid, cv=5, n_jobs=-1)\n",
        "# SVM_model = LinearSVC(C=1, loss=\"hinge\")\n",
        "SVM_model = svm.SVC(kernel=\"linear\", C=1)\n",
        "# fit the model with training dataset\n",
        "SVM_model.fit(train_data_fasttext, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-reb6vBvrkQ",
        "outputId": "6ec0dd57-05b1-49c7-a795-e96ed9b105c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1, kernel='linear')"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predict the result labels on validation dataset\n",
        "predictions_SVM = SVM_model.predict(test_data_fasttext)"
      ],
      "metadata": {
        "id": "QytWJKRtww_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_eval(prediction):\n",
        "\n",
        "    print(\"Confusion_Matrix：\\n\")\n",
        "    print(metrics.confusion_matrix(y_val, prediction))\n",
        "\n",
        "    print(\"Results：\\n\")\n",
        "    print(metrics.classification_report(y_val, prediction))\n"
      ],
      "metadata": {
        "id": "CI2gTKOGx8iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_eval(predictions_SVM)"
      ],
      "metadata": {
        "id": "mnyhlkRqyd80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest"
      ],
      "metadata": {
        "id": "D0UtmNPBw7hT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RF = RandomForestClassifier()\n",
        "RF.fit(train_data_fasttext, y_train)\n",
        "# show model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awCHvB4Qw88i",
        "outputId": "5797a77f-fbbe-4835-b62f-510c5fbe53d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier()"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_RF = RF.predict(test_data_fasttext)"
      ],
      "metadata": {
        "id": "o9SH2GRF_MaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_eval(predictions_RF)"
      ],
      "metadata": {
        "id": "l2T9wok0zHTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression"
      ],
      "metadata": {
        "id": "GSjzUerNw24-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LR = LogisticRegression()\n",
        "LR.fit(train_data_fasttext, y_train)\n",
        "# show model\n"
      ],
      "metadata": {
        "id": "LuaE7Vd-w5Sl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_LR = LR.predict(test_data_fasttext)"
      ],
      "metadata": {
        "id": "ZbA2iary_iAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_eval(predictions_LR)"
      ],
      "metadata": {
        "id": "goaQkbymzT9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# on test dataset\n",
        "cm = metrics.confusion_matrix(y_val, predictions_LR)\n",
        "\n",
        "print('Confusion matrix：', cm, sep='\\n')\n",
        "\n",
        "TN = cm[0][0]\n",
        "FP = cm[0][1]\n",
        "FN = cm[1][0]\n",
        "TP = cm[1][1]\n",
        "\n",
        "# print(FP, FN, TP, TN)\n",
        "\n",
        "# accuracy: (TP + TN) / (TP + TN + FP + FN)\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "print(\"Accuracy: {:.3}%\".format(accuracy*100))\n",
        "\n",
        "# sensitivity(recall rate): TP / (TP + FN)\n",
        "sensitivity = TP / (TP + FN)\n",
        "print(\"Sensitivity: {:.3}%\".format(sensitivity*100))\n",
        "\n",
        "# specificity: TN / (FP + TN)\n",
        "specificity = TN / (FP + TN)\n",
        "print(\"Specificity: {:.3}%\".format(specificity*100))\n",
        "\n",
        "# precision: TP / (TP + FP)\n",
        "precision = TP / (TP + FP)\n",
        "print(\"Precision: {:.3}%\".format(precision*100))\n",
        "\n",
        "# F1 score: 2 * (precision * sensitivity) / (precision + sensitivity)\n",
        "print(\"F1 score: {:.3}%\".format(2 * (precision * sensitivity) / (precision + sensitivity)*100))\n",
        "\n",
        "# classification report\n",
        "r = metrics.classification_report(y_val, predictions_LR)\n",
        "print('Classification report：', r, sep='\\n')"
      ],
      "metadata": {
        "id": "I3cbLzDBxGZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_fasttext"
      ],
      "metadata": {
        "id": "xoZfqfxZ_rjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_fasttext"
      ],
      "metadata": {
        "id": "lcO6_-s0_uvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vBaisH7P_wZE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}