{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MTL_TaskA,B.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN422DgQWsJEnSxND9/135Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hongzf/OLID/blob/main/MTL_TaskA%2CB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCXZz5vRKBQj"
      },
      "outputs": [],
      "source": [
        "!!pip install emoji\n",
        "!pip install wordsegment\n",
        "!pip install transformers\n",
        "!pip install trainer\n",
        "!pip install attention"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import os\n",
        "import emoji\n",
        "from wordsegment import load, segment\n",
        "import pickle\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import random\n",
        "\n",
        "from transformers import BertTokenizer, RobertaTokenizer, get_cosine_schedule_with_warmup\n",
        "from trainer import Trainer \n",
        "from torch import nn\n",
        "from transformers import BertModel, BertForSequenceClassification, RobertaForSequenceClassification, RobertaModel\n",
        "from attention import Attention\n",
        "load()"
      ],
      "metadata": {
        "id": "ElicfwBaKI4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hVxTF1qhKK6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save(toBeSaved, filename, mode='wb'):\n",
        "    dirname = os.path.dirname(filename)\n",
        "    if not os.path.exists(dirname):\n",
        "        os.makedirs(dirname)\n",
        "    file = open(filename, mode)\n",
        "    pickle.dump(toBeSaved, file)\n",
        "    file.close()\n",
        "\n",
        "def load(filename, mode='rb'):\n",
        "    file = open(filename, mode)\n",
        "    loaded = pickle.load(file)\n",
        "    file.close()\n",
        "    return loaded\n",
        "\n",
        "def pad_sents(sents, pad_token):\n",
        "    sents_padded = []\n",
        "    lens = get_lens(sents)\n",
        "    max_len = max(lens)\n",
        "    sents_padded = [sents[i] + [pad_token] * (max_len - l) for i, l in enumerate(lens)]\n",
        "    return sents_padded\n",
        "\n",
        "def sort_sents(sents, reverse=True):\n",
        "    sents.sort(key=(lambda s: len(s)), reverse=reverse)\n",
        "    return sents\n",
        "\n",
        "def get_mask(sents, unmask_idx=1, mask_idx=0):\n",
        "    lens = get_lens(sents)\n",
        "    max_len = max(lens)\n",
        "    mask = [([unmask_idx] * l + [mask_idx] * (max_len - l)) for l in lens]\n",
        "    return mask\n",
        "\n",
        "def get_lens(sents):\n",
        "    return [len(sent) for sent in sents]\n",
        "\n",
        "def get_max_len(sents):\n",
        "    max_len = max([len(sent) for sent in sents])\n",
        "    return max_len\n",
        "\n",
        "def truncate_sents(sents, length):\n",
        "    sents = [sent[:length] for sent in sents]\n",
        "    return sents\n",
        "\n",
        "def get_loss_weight(labels, label_order):\n",
        "    nums = [np.sum(labels == lo) for lo in label_order]\n",
        "    loss_weight = torch.tensor([n / len(labels) for n in nums])\n",
        "    return loss_weight"
      ],
      "metadata": {
        "id": "qYO6K1axKQE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Define the Attention Layer of the model.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Module\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "class Attention(Module):\n",
        "    \"\"\"\n",
        "    Computes a weighted average of the different channels across timesteps.\n",
        "    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, attention_size, return_attention=False):\n",
        "        \"\"\" Initialize the attention layer\n",
        "        # Arguments:\n",
        "            attention_size: Size of the attention vector.\n",
        "            return_attention: If true, output will include the weight for each input token\n",
        "                              used for the prediction\n",
        "        \"\"\"\n",
        "        super(Attention, self).__init__()\n",
        "        self.return_attention = return_attention\n",
        "        self.attention_size = attention_size\n",
        "        self.attention_vector = Parameter(torch.FloatTensor(attention_size))\n",
        "        self.attention_vector.data.normal_(std=0.05) # Initialize attention vector\n",
        "\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def __repr__(self):\n",
        "        s = '{name}({attention_size}, return attention={return_attention})'\n",
        "        return s.format(name=self.__class__.__name__, **self.__dict__)\n",
        "\n",
        "    def forward(self, inputs, input_lengths):\n",
        "        \"\"\" Forward pass.\n",
        "        # Arguments:\n",
        "            inputs (Torch.Variable): Tensor of input sequences\n",
        "            input_lengths (torch.LongTensor): Lengths of the sequences\n",
        "        # Return:\n",
        "            Tuple with (representations and attentions if self.return_attention else None).\n",
        "        \"\"\"\n",
        "        logits = inputs.matmul(self.attention_vector)\n",
        "        unnorm_ai = (logits - logits.max()).exp()\n",
        "\n",
        "        # Compute a mask for the attention on the padded sequences\n",
        "        # See e.g. https://discuss.pytorch.org/t/self-attention-on-words-and-masking/5671/5\n",
        "        max_len = unnorm_ai.size(1)\n",
        "        idxes = torch.arange(0, max_len, out=torch.LongTensor(max_len)).unsqueeze(0).to(device=self.device)\n",
        "        mask = Variable((idxes < input_lengths.unsqueeze(1)).float())\n",
        "\n",
        "        # apply mask and renormalize attention scores (weights)\n",
        "        masked_weights = unnorm_ai * mask\n",
        "        att_sums = masked_weights.sum(dim=1, keepdim=True)  # sums per sequence\n",
        "        attentions = masked_weights.div(att_sums)\n",
        "\n",
        "        # apply attention weights\n",
        "        weighted = torch.mul(inputs, attentions.unsqueeze(-1).expand_as(inputs))\n",
        "\n",
        "        # get the final fixed vector representations of the sentences\n",
        "        representations = weighted.sum(dim=1)\n",
        "\n",
        "        return (representations, attentions if self.return_attention else None)"
      ],
      "metadata": {
        "id": "6Swo99RTKS5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OLID_PATH = './drive/MyDrive/OLID'\n",
        "SAVE_PATH = '.drive/MyDrive/OLID/save'\n",
        "LABEL_DICT = {\n",
        "    'a': {'OFF': 0, 'NOT': 1},\n",
        "    'b': {'TIN': 0, 'UNT': 1, 'NULL': 2}}\n",
        "\n",
        "TRAIN_PATH = os.path.join(OLID_PATH, 'olid-training-v1.0.tsv')"
      ],
      "metadata": {
        "id": "guQ0iHU4KWg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(filepath: str):\n",
        "    df = pd.read_csv(filepath, sep='\\t', keep_default_na=False)\n",
        "    # df= df.sample(frac = 0.9)\n",
        "    ids = np.array(df['id'].values)\n",
        "    tweets = np.array(df['tweet'].values)\n",
        " \n",
        "    # Process tweets\n",
        "    tweets = process_tweets(tweets)\n",
        "\n",
        "    label_a = np.array(df['subtask_a'].values)\n",
        "    label_b = df['subtask_b'].values\n",
        "    # label_c = np.array(df['subtask_c'].values)\n",
        "    nums = len(df)\n",
        "\n",
        "    return nums, ids, tweets, label_a, label_b\n",
        "\n",
        "def read_test_file(task, tokenizer, truncate=512):\n",
        "    df1 = pd.read_csv(os.path.join(OLID_PATH, 'testset-level' + task + '.tsv'), sep='\\t')\n",
        "    df2 = pd.read_csv(os.path.join(OLID_PATH, 'labels-level' + task + '.csv'), sep=',')\n",
        "    # df1 = df1.sample(frac = 0.9)\n",
        "    # df2 = df2.sample(frac = 0.9)\n",
        "    ids = np.array(df1['id'].values)\n",
        "    tweets = np.array(df1['tweet'].values)\n",
        "    labels = np.array(df2['label'].values)\n",
        "    nums = len(df1)\n",
        "\n",
        "    # Process tweets\n",
        "    tweets = process_tweets(tweets)\n",
        "\n",
        "    # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    token_ids = [tokenizer.encode(text=tweets[i], add_special_tokens=True, max_length=truncate) for i in range(nums)]\n",
        "    mask = np.array(get_mask(token_ids))\n",
        "    lens = get_lens(token_ids)\n",
        "    token_ids = np.array(pad_sents(token_ids, tokenizer.pad_token_id))\n",
        "\n",
        "    return ids, token_ids, lens, mask, labels\n",
        "\n",
        "def read_test_file_all(tokenizer, truncate=512):\n",
        "    df = pd.read_csv(os.path.join(OLID_PATH, 'testset-levela.tsv'), sep='\\t')\n",
        "    df_a = pd.read_csv(os.path.join(OLID_PATH, 'labels-levela.csv'), sep=',')\n",
        "    ids = np.array(df['id'].values)\n",
        "    tweets = np.array(df['tweet'].values)\n",
        "    label_a = np.array(df_a['label'].values)\n",
        "    nums = len(df)\n",
        "\n",
        "    # Process tweets\n",
        "    tweets = process_tweets(tweets)\n",
        "\n",
        "    df_b = pd.read_csv(os.path.join(OLID_PATH, 'labels-levelb.csv'), sep=',')\n",
        "    # df_c = pd.read_csv(os.path.join(OLID_PATH, 'labels-levelc.csv'), sep=',')\n",
        "    label_data_b = dict(zip(df_b['id'].values, df_b['label'].values))\n",
        "    # label_data_c = dict(zip(df_c['id'].values, df_c['label'].values))\n",
        "    label_b = [label_data_b[id] if id in label_data_b.keys() else 'NULL' for id in ids]\n",
        "    # label_c = [label_data_c[id] if id in label_data_c.keys() else 'NULL' for id in ids]\n",
        "\n",
        "    token_ids = [tokenizer.encode(text=tweets[i], add_special_tokens=True, max_length=truncate) for i in range(nums)]\n",
        "    mask = np.array(get_mask(token_ids))\n",
        "    lens = get_lens(token_ids)\n",
        "    token_ids = np.array(pad_sents(token_ids, tokenizer.pad_token_id))\n",
        "\n",
        "    return ids, token_ids, lens, mask, label_a, label_b"
      ],
      "metadata": {
        "id": "OcLZUMNfKY51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def task_a(filepath: str, tokenizer, truncate=512):\n",
        "    nums, ids, tweets, label_a, _, _ = read_file(filepath)\n",
        "    # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    token_ids = [tokenizer.encode(text=tweets[i], add_special_tokens=True, max_length=truncate) for i in range(nums)]\n",
        "    mask = np.array(get_mask(token_ids))\n",
        "    lens = get_lens(token_ids)\n",
        "    token_ids = np.array(pad_sents(token_ids, tokenizer.pad_token_id))\n",
        "\n",
        "    return ids, token_ids, lens, mask, label_a\n",
        "\n",
        "def task_b(filepath: str, tokenizer, truncate=512):\n",
        "    nums, ids, tweets, _, label_b, _ = read_file(filepath)\n",
        "    # Only part of the tweets are useful for task b\n",
        "\n",
        "    useful = label_b != 'NULL'\n",
        "    ids = ids[useful]\n",
        "    tweets = tweets[useful]\n",
        "    label_b = label_b[useful]\n",
        "\n",
        "    nums = len(label_b)\n",
        "    # Tokenize\n",
        "    # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    token_ids = [tokenizer.encode(text=tweets[i], add_special_tokens=True, max_length=truncate) for i in range(nums)]\n",
        "    # Get mask\n",
        "    mask = np.array(get_mask(token_ids))\n",
        "    # Get lengths\n",
        "    lens = get_lens(token_ids)\n",
        "    # Pad tokens\n",
        "    token_ids = np.array(pad_sents(token_ids, tokenizer.pad_token_id))\n",
        "\n",
        "    return ids, token_ids, lens, mask, label_b\n",
        "\n",
        "# def task_c(filepath: str, tokenizer, truncate=512):\n",
        "#     nums, ids, tweets, _, _, label_c = read_file(filepath)\n",
        "#     # Only part of the tweets are useful for task c\n",
        "#     useful = label_c != 'NULL'\n",
        "#     ids = ids[useful]\n",
        "#     tweets = tweets[useful]\n",
        "#     label_c = label_c[useful]\n",
        "#     nums = len(label_c)\n",
        "#     # Tokenize\n",
        "#     # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "#     token_ids = [tokenizer.encode(text=tweets[i], add_special_tokens=True, max_length=truncate) for i in range(nums)]\n",
        "#     # Get mask\n",
        "#     mask = np.array(get_mask(token_ids))\n",
        "#     # Get lengths\n",
        "#     lens = get_lens(token_ids)\n",
        "#     # Pad tokens\n",
        "#     token_ids = np.array(pad_sents(token_ids, tokenizer.pad_token_id))\n",
        "\n",
        "#     return ids, token_ids, lens, mask, label_c\n",
        "\n",
        "def all_tasks(filepath: str, tokenizer, truncate=512):\n",
        "    nums, ids, tweets, label_a, label_b = read_file(filepath)\n",
        "    # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    token_ids = [tokenizer.encode(text=tweets[i], add_special_tokens=True, max_length=truncate) for i in range(nums)]\n",
        "    mask = np.array(get_mask(token_ids))\n",
        "    lens = get_lens(token_ids)\n",
        "    token_ids = np.array(pad_sents(token_ids, tokenizer.pad_token_id))\n",
        "\n",
        "    return ids, token_ids, lens, mask, label_a, label_b"
      ],
      "metadata": {
        "id": "jVboomX3Kbm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def emoji2word(sents):\n",
        "    return [emoji.demojize(sent) for sent in sents]\n",
        "\n",
        "def remove_useless_punctuation(sents):\n",
        "    for i, sent in enumerate(sents):\n",
        "        sent = sent.replace(':', ' ')\n",
        "        sent = sent.replace('_', ' ')\n",
        "        sent = sent.replace('...', ' ')\n",
        "        sents[i] = sent\n",
        "    return sents\n",
        "\n",
        "def remove_replicates(sents):\n",
        "    # if there are multiple `@USER` tokens in a tweet, replace it with `@USERS`\n",
        "    # because some tweets contain so many `@USER` which may cause redundant\n",
        "    for i, sent in enumerate(sents):\n",
        "        if sent.find('@USER') != sent.rfind('@USER'):\n",
        "            sents[i] = sent.replace('@USER', '')\n",
        "            sents[i] = '@USERS ' + sents[i]\n",
        "    return sents\n",
        "\n",
        "def replace_rare_words(sents):\n",
        "    rare_words = {\n",
        "        'URL': 'http'\n",
        "    }\n",
        "    for i, sent in enumerate(sents):\n",
        "        for w in rare_words.keys():\n",
        "            sents[i] = sent.replace(w, rare_words[w])\n",
        "    return sents\n",
        "\n",
        "def segment_hashtag(sents):\n",
        "    # E.g. '#LunaticLeft' => 'lunatic left'\n",
        "    for i, sent in enumerate(sents):\n",
        "        sent_tokens = sent.split(' ')\n",
        "        for j, t in enumerate(sent_tokens):\n",
        "            if t.find('#') == 0:\n",
        "                sent_tokens[j] = ' '.join(segment(t))\n",
        "        sents[i] = ' '.join(sent_tokens)\n",
        "    return sents\n",
        "\n",
        "\n",
        "def process_tweets(tweets):\n",
        "    # Process tweets\n",
        "    tweets = emoji2word(tweets)\n",
        "    tweets = replace_rare_words(tweets)\n",
        "    tweets = remove_replicates(tweets)\n",
        "    tweets = segment_hashtag(tweets)\n",
        "    tweets = remove_useless_punctuation(tweets)\n",
        "    tweets = np.array(tweets)\n",
        "    return tweets"
      ],
      "metadata": {
        "id": "Ate298ixKeVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset"
      ],
      "metadata": {
        "id": "D5fzD40-LWEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HuggingfaceDataset(Dataset):\n",
        "    def __init__(self, input_ids, lens, mask, labels, task):\n",
        "        self.input_ids = torch.tensor(input_ids)\n",
        "        self.lens = lens\n",
        "        self.mask = torch.tensor(mask, dtype=torch.float32)\n",
        "        self.labels = labels\n",
        "        self.task = task\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.labels.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        this_LABEL_DICT = LABEL_DICT[self.task]\n",
        "        input = self.input_ids[idx]\n",
        "        length = self.lens[idx]\n",
        "        mask = self.mask[idx]\n",
        "        label = torch.tensor(this_LABEL_DICT[self.labels[idx]])\n",
        "        return input, length, mask, label\n",
        "\n",
        "class HuggingfaceMTDataset(Dataset):\n",
        "    def __init__(self, input_ids, lens, mask, labels, task):\n",
        "        self.input_ids = torch.tensor(input_ids)\n",
        "        self.lens = lens\n",
        "        self.mask = torch.tensor(mask, dtype=torch.float32)\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.labels['a'].shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input = self.input_ids[idx]\n",
        "        mask = self.mask[idx]\n",
        "        length = self.lens[idx]\n",
        "        label_A = torch.tensor(LABEL_DICT['a'][self.labels['a'][idx]])\n",
        "        label_B = torch.tensor(LABEL_DICT['b'][self.labels['b'][idx]])\n",
        "        # label_C = torch.tensor(LABEL_DICT['c'][self.labels['c'][idx]])\n",
        "        return input, length, mask, label_A, label_B\n",
        "\n",
        "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
        "    \"\"\"\n",
        "    Samples elements randomly from a given list of indices for imbalanced dataset\n",
        "    Arguments:\n",
        "        indices (list, optional): a list of indices\n",
        "        num_samples (int, optional): number of samples to draw\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, indices=None, num_samples=None):\n",
        "        # if indices is not provided,\n",
        "        # all elements in the dataset will be considered\n",
        "        self.indices = list(range(len(dataset.labels))) \\\n",
        "            if indices is None else indices\n",
        "\n",
        "        # if num_samples is not provided,\n",
        "        # draw `len(indices)` samples in each iteration\n",
        "        self.num_samples = len(self.indices) \\\n",
        "            if num_samples is None else num_samples\n",
        "\n",
        "        # distribution of classes in the dataset\n",
        "        label_to_count = {}\n",
        "        for idx in self.indices:\n",
        "            label = self._get_label(dataset, idx)\n",
        "            if label in label_to_count:\n",
        "                label_to_count[label] += 1\n",
        "            else:\n",
        "                label_to_count[label] = 1\n",
        "\n",
        "        # weight for each sample\n",
        "        weights = [1.0 / label_to_count[self._get_label(dataset, idx)] for idx in self.indices]\n",
        "        self.weights = torch.DoubleTensor(weights)\n",
        "\n",
        "    def _get_label(self, dataset, id_):\n",
        "        return dataset.labels[id_]\n",
        "\n",
        "    def __iter__(self):\n",
        "        return (self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples"
      ],
      "metadata": {
        "id": "MXZNvlObLVyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "Pc7d_ck-Lhud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#BERT\n",
        "class BERT(nn.Module):\n",
        "    def __init__(self, model_size, num_labels=2):\n",
        "        super(BERT, self).__init__()\n",
        "        self.model = BertForSequenceClassification.from_pretrained(\n",
        "            f'bert-{model_size}-uncased',\n",
        "            num_labels=num_labels,\n",
        "            hidden_dropout_prob= 0.1,\n",
        "            attention_probs_dropout_prob= 0.1\n",
        "        )\n",
        "\n",
        "        # Freeze embeddings' parameters for saving memory\n",
        "        # for param in self.model.bert.embeddings.parameters():\n",
        "        #     param.requires_grad = False\n",
        "\n",
        "    def forward(self, inputs, lens, mask, labels=None):\n",
        "        outputs = self.model(inputs, attention_mask=mask)\n",
        "        logits = outputs[0]\n",
        "        # return loss, logits\n",
        "        return logits\n",
        "\n",
        "class RoBERTa(nn.Module):\n",
        "    def __init__(self, model_size, num_labels=2):\n",
        "        super(RoBERTa, self).__init__()\n",
        "        self.model = RobertaForSequenceClassification.from_pretrained(\n",
        "            f'roberta-{model_size}',\n",
        "            num_labels=num_labels,\n",
        "            hidden_dropout_prob= 0.1,\n",
        "            attention_probs_dropout_prob= 0.1\n",
        "        )\n",
        "\n",
        "        # Freeze embeddings' parameters for saving memory\n",
        "        # for param in self.model.roberta.embeddings.parameters():\n",
        "        #     param.requires_grad = False\n",
        "\n",
        "    def forward(self, inputs, lens, mask, labels):\n",
        "        outputs = self.model(inputs, attention_mask=mask, labels=labels)\n",
        "        loss, logits = outputs[:2]\n",
        "        # return loss, logits\n",
        "        return logits\n",
        "\n",
        "class MTModel(nn.Module):\n",
        "    def __init__(self, model, model_size):\n",
        "        super(MTModel, self).__init__()\n",
        "        if model == 'bert':\n",
        "            pretrained = BertForSequenceClassification.from_pretrained(\n",
        "                f'bert-{model_size}-uncased',\n",
        "                hidden_dropout_prob= 0.1,\n",
        "                attention_probs_dropout_prob= 0.1\n",
        "            )\n",
        "            self.main = pretrained.bert\n",
        "            self.dropout = pretrained.dropout\n",
        "        elif model == 'roberta':\n",
        "            pretrained = RobertaForSequenceClassification.from_pretrained(\n",
        "                f'roberta-{model_size}',\n",
        "                hidden_dropout_prob= 0.1,\n",
        "                attention_probs_dropout_prob= 0.1\n",
        "            )\n",
        "            self.main = pretrained.roberta\n",
        "            self.dropout = pretrained.dropout\n",
        "\n",
        "        # Freeze embeddings' parameters for saving memory\n",
        "        for param in self.main.embeddings.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        linear_in_features = 768 if model_size == 'base' else 1024\n",
        "\n",
        "        self.classifier_a = nn.Linear(in_features=linear_in_features, out_features=2, bias=True)\n",
        "        self.classifier_b = nn.Linear(in_features=linear_in_features, out_features=3, bias=True)\n",
        "        # self.classifier_c = nn.Linear(in_features=linear_in_features, out_features=4, bias=True)\n",
        "\n",
        "    def forward(self, inputs, lens, mask):\n",
        "        outputs = self.main(inputs, attention_mask=mask)\n",
        "        pooled_output = outputs[1]\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        # logits for 3 sub-tasks\n",
        "        logits_A = self.classifier_a(pooled_output)\n",
        "        logits_B = self.classifier_b(pooled_output)\n",
        "        # logits_C = self.classifier_c(pooled_output)\n",
        "        return logits_A, logits_B\n",
        "\n",
        "class BERT_LSTM(nn.Module):\n",
        "    def __init__(self, model_size, num_labels):\n",
        "        super(BERT_LSTM, self).__init__()\n",
        "        hidden_size = 300\n",
        "        self.concat = 'concat'\n",
        "        input_size = 768 if model_size == 'base' else 1024\n",
        "\n",
        "        self.emb = BertModel.from_pretrained(\n",
        "            f'bert-{model_size}-uncased',\n",
        "            hidden_dropout_prob= 0.1,\n",
        "            attention_probs_dropout_prob= 0.1\n",
        "        )\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers= 1,\n",
        "            bidirectional=True,\n",
        "            batch_first=True,\n",
        "            dropout= 0.1 if num_layers > 1 else 0\n",
        "        )\n",
        "        self.dropout = nn.Dropout(p= dropout)\n",
        "        self.linear = nn.Linear(in_features=hidden_size * 2 if self.concat else hidden_size, out_features=num_labels)\n",
        "\n",
        "    def forward(self, inputs, lens, mask, labels):\n",
        "        embs = self.emb(inputs, attention_mask=mask)[0] # (batch_size, sequence_length, hidden_size)\n",
        "        _, (h_n, _) = self.lstm(input=embs) # (num_layers * num_directions, batch, hidden_size)\n",
        "        if self.concat:\n",
        "            h_n = torch.cat((h_n[0], h_n[1]), dim=1)\n",
        "        else:\n",
        "            h_n = h_n[0] + h_n[1]\n",
        "        h_n = self.dropout(h_n)\n",
        "        logits = self.linear(h_n)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "dy_YwaFHLgpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MTL\n",
        "class MTL_Transformer_LSTM(nn.Module):\n",
        "    def __init__(self, model, model_size):\n",
        "        super(MTL_Transformer_LSTM, self).__init__()\n",
        "        hidden_size = 300\n",
        "        self.concat = 'concat'\n",
        "        input_size = 768 if model_size == 'base' else 1024\n",
        "\n",
        "        if model == 'bert':\n",
        "            MODEL = BertModel\n",
        "            model_full_name = f'{model}-{model_size}-uncased'\n",
        "        elif model == 'roberta':\n",
        "            MODEL = RobertaModel\n",
        "            model_full_name = f'{model}-{model_size}'\n",
        "\n",
        "        self.emb = MODEL.from_pretrained(\n",
        "            model_full_name,\n",
        "            hidden_dropout_prob= 0.1,\n",
        "            attention_probs_dropout_prob= 0.1\n",
        "        )\n",
        "\n",
        "        self.LSTMs = nn.ModuleDict({\n",
        "            'a': nn.LSTM(\n",
        "                input_size=input_size,\n",
        "                hidden_size=hidden_size,\n",
        "                num_layers= 1,\n",
        "                bidirectional=True,\n",
        "                batch_first=True,\n",
        "                dropout= 0.1 if num_layers > 1 else 0\n",
        "            ),\n",
        "            'b': nn.LSTM(\n",
        "                input_size=input_size,\n",
        "                hidden_size=hidden_size,\n",
        "                num_layers= 1,\n",
        "                bidirectional=True,\n",
        "                batch_first=True,\n",
        "                dropout= 0.1 if num_layers > 1 else 0\n",
        "            ),\n",
        "            # 'c': nn.LSTM(\n",
        "            #     input_size=input_size,\n",
        "            #     hidden_size=hidden_size,\n",
        "            #     num_layers= 1,\n",
        "            #     bidirectional=True,\n",
        "            #     batch_first=True,\n",
        "            #     dropout= 0.1 if num_layers > 1 else 0\n",
        "            # )\n",
        "        })\n",
        "\n",
        "        self.attention_layers = nn.ModuleDict({\n",
        "            'a': Attention(hidden_size * 2),\n",
        "            'b': Attention(hidden_size * 2),\n",
        "            # 'c': Attention(hidden_size * 2)\n",
        "        })\n",
        "\n",
        "        self.dropout = nn.Dropout(p= dropout)\n",
        "\n",
        "        linear_in_features = hidden_size * 2 if self.concat else hidden_size\n",
        "        self.Linears = nn.ModuleDict({\n",
        "            'a': nn.Sequential(\n",
        "                nn.Linear(linear_in_features, hidden_size),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_size, 2)\n",
        "            ),\n",
        "            'b': nn.Sequential(\n",
        "                nn.Linear(linear_in_features, hidden_size),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_size, 3)\n",
        "            ),\n",
        "            # 'c': nn.Sequential(\n",
        "            #     nn.Linear(linear_in_features, hidden_size),\n",
        "            #     nn.ReLU(),\n",
        "            #     nn.Linear(hidden_size, 4)\n",
        "            # )\n",
        "        })\n",
        "\n",
        "    def forward(self, inputs, lens, mask):\n",
        "        embs = self.emb(inputs, attention_mask=mask)[0] # (batch_size, sequence_length, hidden_size)\n",
        "\n",
        "        _, (h_a, _) = self.LSTMs['a'](embs)\n",
        "        if self.concat:\n",
        "            h_a = torch.cat((h_a[0], h_a[1]), dim=1)\n",
        "        else:\n",
        "            h_a = h_a[0] + h_a[1]\n",
        "        h_a = self.dropout(h_a)\n",
        "\n",
        "        _, (h_b, _) = self.LSTMs['b'](embs)\n",
        "        if self.concat:\n",
        "            h_b = torch.cat((h_b[0], h_b[1]), dim=1)\n",
        "        else:\n",
        "            h_b = h_b[0] + h_b[1]\n",
        "        h_b = self.dropout(h_b)\n",
        "\n",
        "        # _, (h_c, _) = self.LSTMs['c'](embs)\n",
        "        # if self.concat:\n",
        "        #     h_c = torch.cat((h_c[0], h_c[1]), dim=1)\n",
        "        # else:\n",
        "        #     h_c = h_c[0] + h_c[1]\n",
        "        # h_c = self.dropout(h_c)\n",
        "\n",
        "        logits_a = self.Linears['a'](h_a)\n",
        "        logits_b = self.Linears['b'](h_b)\n",
        "        # logits_c = self.Linears['c'](h_c)\n",
        "\n",
        "        return logits_a, logits_b"
      ],
      "metadata": {
        "id": "poTfptZZLoun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = 'all'\n",
        "model_name = 'bert'\n",
        "model_size = 'base'\n",
        "truncate = 512 #100 #512\n",
        "epochs = 30\n",
        "lr = 1e-4 # == 0.00001~0.00007\n",
        "wd = 0.0\n",
        "bs = 32 #128 #180 #72\n",
        "patience = 5\n",
        "num_layers= 1\n",
        "dropout = 0.1"
      ],
      "metadata": {
        "id": "ffk22kmbLsbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix seed for reproducibility\n",
        "seed = 19951126\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Set device\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "num_labels = 2 if task == 'c' else 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prwWnVqrLtwp",
        "outputId": "7eaefcc8-c207-4e23-8faf-4195eea1ee70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#run model and choose tokenzier\n",
        "\n",
        "\n",
        "if model_name == 'bert':\n",
        "        if task == 'all':\n",
        "            model = MTL_Transformer_LSTM(model_name, model_size)\n",
        "        else:\n",
        "            model = BERT(model_size, num_labels=num_labels)\n",
        "        tokenizer = BertTokenizer.from_pretrained(f'bert-{model_size}-uncased')\n",
        "elif model_name == 'roberta':\n",
        "        if task == 'all':\n",
        "            model = MTL_Transformer_LSTM(model_name, model_size)\n",
        "        else:\n",
        "            model = RoBERTa(model_size, num_labels=num_labels)\n",
        "        tokenizer = RobertaTokenizer.from_pretrained(f'roberta-{model_size}')\n",
        "elif model_name == 'bert-gate' and task == 'all':\n",
        "        model_name = model_name.replace('-gate', '')\n",
        "        model = GatedModel(model_name, model_size)\n",
        "        tokenizer = BertTokenizer.from_pretrained(f'bert-{model_size}-uncased')\n",
        "elif model_name == 'roberta-gate' and task == 'all':\n",
        "        model_name = model_name.replace('-gate', '')\n",
        "        model = GatedModel(model_name, model_size)\n",
        "        tokenizer = RobertaTokenizer.from_pretrained(f'roberta-{model_size}')\n",
        "\n",
        "# Move model to correct device\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "yDsVg_zTLxUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if task in ['a', 'b', 'c']:\n",
        "        data_methods = {'a': task_a, 'b': task_b, 'c': task_c}\n",
        "        ids, token_ids, lens, mask, labels = data_methods[task](TRAIN_PATH, tokenizer=tokenizer, truncate=truncate)\n",
        "        test_ids, test_token_ids, test_lens, test_mask, test_labels = read_test_file(task, tokenizer=tokenizer, truncate=truncate)\n",
        "        _Dataset = HuggingfaceDataset\n",
        "elif task in ['all']:\n",
        "        ids, token_ids, lens, mask, label_a, label_b = all_tasks(TRAIN_PATH, tokenizer=tokenizer, truncate=truncate)\n",
        "        test_ids, test_token_ids, test_lens, test_mask, test_label_a, test_label_b = read_test_file_all(tokenizer)\n",
        "        labels = {'a': label_a, 'b': label_b}\n",
        "        test_labels = {'a': test_label_a, 'b': test_label_b}\n",
        "        _Dataset = HuggingfaceMTDataset\n"
      ],
      "metadata": {
        "id": "oqn47VyZLzKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Built-in libraries\n",
        "import copy\n",
        "import datetime\n",
        "from typing import Dict, List\n",
        "# Third-party libraries\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "import csv"
      ],
      "metadata": {
        "id": "Kd563hHzL0wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer():\n",
        "    '''\n",
        "    The trainer for training models.\n",
        "    It can be used for both single and multi task training.\n",
        "    Every class function ends with _m is for multi-task training.\n",
        "    '''\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        epochs: int,\n",
        "        dataloaders: Dict[str, DataLoader],\n",
        "        criterion: nn.Module,\n",
        "        loss_weights: List[float],\n",
        "        clip: bool,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        scheduler: torch.optim.lr_scheduler,\n",
        "        device: str,\n",
        "        patience: int,\n",
        "        task_name: str,\n",
        "        model_name: str,\n",
        "        seed: int\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.epochs = epochs\n",
        "        self.dataloaders = dataloaders\n",
        "        self.criterion = criterion\n",
        "        self.loss_weights = loss_weights\n",
        "        self.clip = clip\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.device = device\n",
        "        self.patience = patience\n",
        "        self.task_name = task_name\n",
        "        self.model_name = model_name\n",
        "        self.seed = seed\n",
        "        self.datetimestr = datetime.datetime.now().strftime('%Y-%b-%d_%H:%M:%S')\n",
        "\n",
        "        # Evaluation results\n",
        "        self.train_losses = []\n",
        "        self.test_losses = []\n",
        "        self.train_f1 = []\n",
        "        self.test_f1 = []\n",
        "        self.best_train_f1 = 0.0\n",
        "        self.best_test_f1 = 0.0\n",
        "\n",
        "        # Evaluation results for multi-task\n",
        "        self.best_train_f1_m = np.array([0, 0, 0], dtype=np.float64)\n",
        "        self.best_test_f1_m = np.array([0, 0, 0], dtype=np.float64)\n",
        "\n",
        "    def train(self):\n",
        "        for epoch in range(self.epochs):\n",
        "            print(f'Epoch {epoch}')\n",
        "            print('=' * 20)\n",
        "            self.train_one_epoch()\n",
        "            self.test()\n",
        "            print(f'Best test f1: {self.best_test_f1:.4f}')\n",
        "            print('=' * 20)\n",
        "            \n",
        "            if epoch%3 == 0:\n",
        "              print('Saving results ...')\n",
        "              save(\n",
        "                  (self.train_losses, self.test_losses, self.train_f1, self.test_f1, self.best_train_f1, self.best_test_f1),\n",
        "                  f'./drive/MyDrive/OLID/save/results/single_{self.task_name}_{self.datetimestr}_{self.best_test_f1:.4f}.pt'\n",
        "              )\n",
        "\n",
        "    def train_one_epoch(self):\n",
        "        self.model.train()\n",
        "        dataloader = self.dataloaders['train']\n",
        "        y_pred_all = None\n",
        "        labels_all = None\n",
        "        loss = 0\n",
        "        iters_per_epoch = 0\n",
        "        for inputs, lens, mask, labels in tqdm(dataloader, desc='Training'):\n",
        "            iters_per_epoch += 1\n",
        "\n",
        "            if labels_all is None:\n",
        "                labels_all = labels.numpy()\n",
        "            else:\n",
        "                labels_all = np.concatenate((labels_all, labels.numpy()))\n",
        "\n",
        "            inputs = inputs.to(device=self.device)\n",
        "            lens = lens.to(device=self.device)\n",
        "            mask = mask.to(device=self.device)\n",
        "            labels = labels.to(device=self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            with torch.set_grad_enabled(True):\n",
        "                # Forward\n",
        "                logits = self.model(inputs, lens, mask, labels)\n",
        "                _loss = self.criterion(logits, labels)\n",
        "                loss += _loss.item()\n",
        "                y_pred = logits.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "                if y_pred_all is None:\n",
        "                    y_pred_all = y_pred\n",
        "                else:\n",
        "                    y_pred_all = np.concatenate((y_pred_all, y_pred))\n",
        "\n",
        "                # Backward\n",
        "                _loss.backward()\n",
        "                if self.clip:\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10)\n",
        "                self.optimizer.step()\n",
        "                if self.scheduler is not None:\n",
        "                    self.scheduler.step()\n",
        "\n",
        "            loss /= iters_per_epoch\n",
        "            f1 = f1_score(labels_all, y_pred_all, average='macro')\n",
        "\n",
        "            print(f'loss = {loss:.4f}')\n",
        "            print(f'Macro-F1 = {f1:.4f}')\n",
        "\n",
        "            self.train_losses.append(loss)\n",
        "            self.train_f1.append(f1)\n",
        "            if f1 > self.best_train_f1:\n",
        "                self.best_train_f1 = f1\n",
        "\n",
        "    def test(self):\n",
        "        self.model.eval()\n",
        "        dataloader = self.dataloaders['test']\n",
        "        y_pred_all = None\n",
        "        labels_all = None\n",
        "        loss = 0\n",
        "        iters_per_epoch = 0\n",
        "        for inputs, lens, mask, labels in tqdm(dataloader, desc='Testing'):\n",
        "            iters_per_epoch += 1\n",
        "\n",
        "            if labels_all is None:\n",
        "                labels_all = labels.numpy()\n",
        "            else:\n",
        "                labels_all = np.concatenate((labels_all, labels.numpy()))\n",
        "\n",
        "            inputs = inputs.to(device=self.device)\n",
        "            lens = lens.to(device=self.device)\n",
        "            mask = mask.to(device=self.device)\n",
        "            labels = labels.to(device=self.device)\n",
        "\n",
        "            with torch.set_grad_enabled(False):\n",
        "                logits = self.model(inputs, lens, mask, labels)\n",
        "                _loss = self.criterion(logits, labels)\n",
        "                y_pred = logits.argmax(dim=1).cpu().numpy()\n",
        "                loss += _loss.item()\n",
        "\n",
        "                if y_pred_all is None:\n",
        "                    y_pred_all = y_pred\n",
        "                else:\n",
        "                    y_pred_all = np.concatenate((y_pred_all, y_pred))\n",
        "\n",
        "            loss /= iters_per_epoch\n",
        "            f1 = f1_score(labels_all, y_pred_all, average='macro')\n",
        "\n",
        "            print(f'loss = {loss:.4f}')\n",
        "            print(f'Macro-F1 = {f1:.4f}')\n",
        "\n",
        "            self.test_losses.append(loss)\n",
        "            self.test_f1.append(f1)\n",
        "            if f1 > self.best_test_f1:\n",
        "                self.best_test_f1 = f1\n",
        "                self.save_model()\n",
        "\n",
        "    def train_m(self):\n",
        "        for epoch in range(self.epochs):\n",
        "            print(f'Epoch {epoch}')\n",
        "            print('=' * 20)\n",
        "            self.train_one_epoch_m()\n",
        "            self.test_m()\n",
        "            print(f'Best test results A: {self.best_test_f1_m[0]:.4f}')\n",
        "            print(f'Best test results B: {self.best_test_f1_m[1]:.4f}')\n",
        "            # print(f'Best test results C: {self.best_test_f1_m[2]:.4f}')\n",
        "            print('=' * 20)\n",
        "\n",
        "        print('Saving results ...')\n",
        "        save(\n",
        "            (self.train_losses, self.test_losses, self.train_f1, self.test_f1, self.best_train_f1_m, self.best_test_f1_m),\n",
        "            f'./drive/MyDrive/OLID/save/results/mtl_{self.datetimestr}_{self.best_test_f1_m[0]:.4f}.pt'\n",
        "        )\n",
        "\n",
        "    def train_one_epoch_m(self):\n",
        "        self.model.train()\n",
        "        dataloader = self.dataloaders['train']\n",
        "\n",
        "        y_pred_all_A = None\n",
        "        y_pred_all_B = None\n",
        "        labels_all_A = None\n",
        "        labels_all_B = None\n",
        "\n",
        "\n",
        "        loss = 0\n",
        "        iters_per_epoch = 0\n",
        "        for inputs, lens, mask, label_A, label_B in tqdm(dataloader, desc='Training M'):\n",
        "            iters_per_epoch += 1\n",
        "\n",
        "            inputs = inputs.to(device=self.device)\n",
        "            lens = lens.to(device=self.device)\n",
        "            mask = mask.to(device=self.device)\n",
        "            label_A = label_A.to(device=self.device)\n",
        "            label_B = label_B.to(device=self.device)\n",
        "            # label_C = label_C.to(device=self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            with torch.set_grad_enabled(True):\n",
        "                # Forward\n",
        "                # logits_A, logits_B, logits_C = self.model(inputs, mask)\n",
        "                all_logits = self.model(inputs, lens, mask)\n",
        "                y_pred_A = all_logits[0].argmax(dim=1).cpu().numpy()\n",
        "                y_pred_B = all_logits[1][:, 0:2].argmax(dim=1)\n",
        "                # y_pred_C = all_logits[2][:, 0:3].argmax(dim=1)\n",
        "\n",
        "                Non_null_index_B = label_B != LABEL_DICT['b']['NULL']\n",
        "                Non_null_label_B = label_B[Non_null_index_B]\n",
        "                Non_null_pred_B = y_pred_B[Non_null_index_B]\n",
        "\n",
        "                # Non_null_index_C = label_C != LABEL_DICT['c']['NULL']\n",
        "                # Non_null_label_C = label_C[Non_null_index_C]\n",
        "                # Non_null_pred_C = y_pred_C[Non_null_index_C]\n",
        "\n",
        "                labels_all_A = label_A.cpu().numpy() if labels_all_A is None else np.concatenate((labels_all_A, label_A.cpu().numpy()))\n",
        "                labels_all_B = Non_null_label_B.cpu().numpy() if labels_all_B is None else np.concatenate((labels_all_B, Non_null_label_B.cpu().numpy()))\n",
        "                # labels_all_C = Non_null_label_C.cpu().numpy() if labels_all_C is None else np.concatenate((labels_all_C, Non_null_label_C.cpu().numpy()))\n",
        "\n",
        "                y_pred_all_A = y_pred_A if y_pred_all_A is None else np.concatenate((y_pred_all_A, y_pred_A))\n",
        "                y_pred_all_B = Non_null_pred_B.cpu().numpy() if y_pred_all_B is None else np.concatenate((y_pred_all_B, Non_null_pred_B.cpu().numpy()))\n",
        "                # y_pred_all_C = Non_null_pred_C.cpu().numpy() if y_pred_all_C is None else np.concatenate((y_pred_all_C, Non_null_pred_C.cpu().numpy()))\n",
        "\n",
        "                # f1[0] += self.calc_f1(label_A, y_pred_A)\n",
        "                # f1[1] += self.calc_f1(Non_null_label_B, Non_null_pred_B)\n",
        "                # f1[2] += self.calc_f1(Non_null_label_C, Non_null_pred_C)\n",
        "\n",
        "                _loss = self.loss_weights[0] * self.criterion(all_logits[0], label_A)\n",
        "                _loss += self.loss_weights[1] * self.criterion(all_logits[1], label_B)\n",
        "                # _loss += self.loss_weights[2] * self.criterion(all_logits[2], label_C)\n",
        "                loss += _loss.item()\n",
        "\n",
        "                # Backward\n",
        "                _loss.backward()\n",
        "                if self.clip:\n",
        "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10)\n",
        "                self.optimizer.step()\n",
        "                if self.scheduler is not None:\n",
        "                    self.scheduler.step()\n",
        "\n",
        "        loss /= iters_per_epoch\n",
        "        f1_A = f1_score(labels_all_A, y_pred_all_A, average='macro')\n",
        "        f1_B = f1_score(labels_all_B, y_pred_all_B, average='macro')\n",
        "        # f1_C = f1_score(labels_all_C, y_pred_all_C, average='macro')\n",
        "\n",
        "        print(f'loss = {loss:.4f}')\n",
        "        print(f'A: {f1_A:.4f}')\n",
        "        print(f'B: {f1_B:.4f}')\n",
        "        # print(f'C: {f1_C:.4f}')\n",
        "\n",
        "        self.train_losses.append(loss)\n",
        "        self.train_f1.append([f1_A, f1_B])\n",
        "\n",
        "        if f1_A > self.best_train_f1_m[0]:\n",
        "            self.best_train_f1_m[0] = f1_A\n",
        "        if f1_B > self.best_train_f1_m[1]:\n",
        "            self.best_train_f1_m[1] = f1_B\n",
        "        # if f1_C > self.best_train_f1_m[2]:\n",
        "        #     self.best_train_f1_m[2] = f1_C\n",
        "\n",
        "    def test_m(self):\n",
        "        self.model.eval()\n",
        "        dataloader = self.dataloaders['test']\n",
        "        loss = 0\n",
        "        iters_per_epoch = 0\n",
        "\n",
        "        y_pred_all_A = None\n",
        "        y_pred_all_B = None\n",
        "        # y_pred_all_C = None\n",
        "        labels_all_A = None\n",
        "        labels_all_B = None\n",
        "        # labels_all_C = None\n",
        "\n",
        "        for inputs, lens, mask, label_A, label_B in tqdm(dataloader, desc='Test M'):\n",
        "            iters_per_epoch += 1\n",
        "\n",
        "            labels_all_A = label_A.numpy() if labels_all_A is None else np.concatenate((labels_all_A, label_A.numpy()))\n",
        "            labels_all_B = label_B.numpy() if labels_all_B is None else np.concatenate((labels_all_B, label_B.numpy()))\n",
        "            # labels_all_C = label_C.numpy() if labels_all_C is None else np.concatenate((labels_all_C, label_C.numpy()))\n",
        "\n",
        "            inputs = inputs.to(device=self.device)\n",
        "            lens = lens.to(device=self.device)\n",
        "            mask = mask.to(device=self.device)\n",
        "            label_A = label_A.to(device=self.device)\n",
        "            label_B = label_B.to(device=self.device)\n",
        "            # label_C = label_C.to(device=self.device)\n",
        "\n",
        "            with torch.set_grad_enabled(False):\n",
        "                all_logits = self.model(inputs, lens, mask)\n",
        "                y_pred_A = all_logits[0].argmax(dim=1).cpu().numpy()\n",
        "                y_pred_B = all_logits[1].argmax(dim=1).cpu().numpy()\n",
        "                # y_pred_C = all_logits[2].argmax(dim=1).cpu().numpy()\n",
        "\n",
        "                # f1[0] += self.calc_f1(label_A, y_pred_A)\n",
        "                # f1[1] += self.calc_f1(label_B, y_pred_B)\n",
        "                # f1[2] += self.calc_f1(label_C, y_pred_C)\n",
        "\n",
        "                y_pred_all_A = y_pred_A if y_pred_all_A is None else np.concatenate((y_pred_all_A, y_pred_A))\n",
        "                y_pred_all_B = y_pred_B if y_pred_all_B is None else np.concatenate((y_pred_all_B, y_pred_B))\n",
        "                # y_pred_all_C = y_pred_C if y_pred_all_C is None else np.concatenate((y_pred_all_C, y_pred_C))\n",
        "\n",
        "                _loss = self.loss_weights[0] * self.criterion(all_logits[0], label_A)\n",
        "                _loss += self.loss_weights[1] * self.criterion(all_logits[1], label_B)\n",
        "                # _loss += self.loss_weights[2] * self.criterion(all_logits[2], label_C)\n",
        "                loss += _loss.item()\n",
        "\n",
        "        loss /= iters_per_epoch\n",
        "        f1_A = f1_score(labels_all_A, y_pred_all_A, average='macro')\n",
        "        f1_B = f1_score(labels_all_B, y_pred_all_B, average='macro')\n",
        "        # f1_C = f1_score(labels_all_C, y_pred_all_C, average='macro')\n",
        "\n",
        "        print(f'loss = {loss:.4f}')\n",
        "        print(f'A: {f1_A:.4f}')\n",
        "        print(f'B: {f1_B:.4f}')\n",
        "        # print(f'C: {f1_C:.4f}')\n",
        "\n",
        "        self.test_losses.append(loss)\n",
        "        self.test_f1.append([f1_A, f1_B])\n",
        "\n",
        "        if f1_A > self.best_test_f1_m[0]:\n",
        "            self.best_test_f1_m[0] = f1_A\n",
        "            self.save_model()\n",
        "        if f1_B > self.best_test_f1_m[1]:\n",
        "            self.best_test_f1_m[1] = f1_B\n",
        "        # if f1_C > self.best_test_f1_m[2]:\n",
        "        #     self.best_test_f1_m[2] = f1_C\n",
        "\n",
        "        # for i in range(len(f1)):\n",
        "        #     for j in range(len(f1[0])):\n",
        "        #         if f1[i][j] > self.best_test_f1_m[i][j]:\n",
        "        #             self.best_test_f1_m[i][j] = f1[i][j]\n",
        "        #             if i == 0 and j == 0:\n",
        "        #                 self.save_model()\n",
        "\n",
        "    def calc_f1(self, labels, y_pred):\n",
        "        return np.array([\n",
        "            f1_score(labels.cpu(), y_pred.cpu(), average='macro'),\n",
        "            f1_score(labels.cpu(), y_pred.cpu(), average='micro'),\n",
        "            f1_score(labels.cpu(), y_pred.cpu(), average='weighted')\n",
        "        ], np.float64)\n",
        "\n",
        "    def printing(self, loss, f1):\n",
        "        print(f'loss = {loss:.4f}')\n",
        "        print(f'Macro-F1 = {f1[0]:.4f}')\n",
        "        # print(f'Micro-F1 = {f1[1]:.4f}')\n",
        "        # print(f'Weighted-F1 = {f1[2]:.4f}')\n",
        "\n",
        "    def save_model(self):\n",
        "        print('Saving model...')\n",
        "        if self.task_name == 'all':\n",
        "            filename = f'./drive/MyDrive/OLID/save/models/{self.task_name}_{self.model_name}_{self.best_test_f1_m[0]}_seed{self.seed}.pt'\n",
        "        else:\n",
        "            filename = f'./drive/MyDrive/OLID/save/models/{self.task_name}_{self.model_name}_{self.best_test_f1}_seed{self.seed}.pt'\n",
        "        save(copy.deepcopy(self.model.state_dict()), filename)"
      ],
      "metadata": {
        "id": "jqDYPTqFL6aI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = {\n",
        "        'train': _Dataset(\n",
        "            input_ids=token_ids,\n",
        "            lens=lens,\n",
        "            mask=mask,\n",
        "            labels=labels,\n",
        "            task=task\n",
        "        ),\n",
        "        'test': _Dataset(\n",
        "            input_ids=test_token_ids,\n",
        "            lens=test_lens,\n",
        "            mask=test_mask,\n",
        "            labels=test_labels,\n",
        "            task=task\n",
        "        )\n",
        "    }\n",
        "\n",
        "sampler = ImbalancedDatasetSampler(datasets['train']) if task in ['a', 'b'] else None\n",
        "dataloaders = {\n",
        "    'train': DataLoader(\n",
        "        dataset=datasets['train'],\n",
        "        batch_size=bs,\n",
        "        sampler=sampler\n",
        "    ),\n",
        "    'test': DataLoader(dataset=datasets['test'], batch_size=bs)\n",
        "    }\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "if True:\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "        # A warmup scheduler\n",
        "        t_total = epochs * len(dataloaders['train'])\n",
        "        warmup_steps = np.ceil(t_total / 10.0) * 2\n",
        "        scheduler = get_cosine_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=warmup_steps,\n",
        "            num_training_steps=t_total\n",
        "        )\n",
        "else:\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "        scheduler = None"
      ],
      "metadata": {
        "id": "ydkvgnsRM8Rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "        model=model,\n",
        "        epochs=epochs,\n",
        "        dataloaders=dataloaders,\n",
        "        criterion=criterion,\n",
        "        loss_weights=[0.543, 0.457],\n",
        "        clip= True,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        device=device,\n",
        "        patience=patience,\n",
        "        task_name=task,\n",
        "        model_name=model_name, \n",
        "        seed=19951126\n",
        "    )\n",
        "if task in ['a', 'b', 'c']:\n",
        "    trainer.train()\n",
        "else:\n",
        "    trainer.train_m()"
      ],
      "metadata": {
        "id": "F14zL7XnPUzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "        model=model,\n",
        "        epochs=epochs,\n",
        "        dataloaders=dataloaders,\n",
        "        criterion=criterion,\n",
        "        loss_weights=[0.544, 0.456],\n",
        "        clip= True,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        device=device,\n",
        "        patience=patience,\n",
        "        task_name=task,\n",
        "        model_name=model_name, \n",
        "        seed=19951126\n",
        "    )\n",
        "if task in ['a', 'b', 'c']:\n",
        "    trainer.train()\n",
        "else:\n",
        "    trainer.train_m()"
      ],
      "metadata": {
        "id": "Pgxs0Ii_PaZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wSaYwsLQr4U6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}