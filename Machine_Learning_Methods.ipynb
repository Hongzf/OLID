{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine_Learning_Methods.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMi8C6/byR7B+XiMIG/s72J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hongzf/OLID/blob/main/Machine_Learning_Methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCz94PP_yJ8z"
      },
      "outputs": [],
      "source": [
        "!pip install emoji\n",
        "!pip install wordsegment\n",
        "!pip install transformers\n",
        "!pip install trainer\n",
        "!pip install attention"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import os\n",
        "import emoji\n",
        "from wordsegment import load, segment\n",
        "import pickle\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import os,re\n",
        "from bs4 import BeautifulSoup\n",
        "from gensim import models\n",
        "\n",
        "from transformers import BertTokenizer, RobertaTokenizer, get_cosine_schedule_with_warmup\n",
        "from trainer import Trainer \n",
        "from torch import nn\n",
        "from transformers import BertModel, BertForSequenceClassification, RobertaForSequenceClassification, RobertaModel\n",
        "from attention import Attention\n",
        "load()"
      ],
      "metadata": {
        "id": "tq0b2L4T3yfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#optional Google drive integration\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjY4WW5w4AOy",
        "outputId": "0508e32d-689a-424b-cc77-d917bb0ff1b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OLID_PATH = './drive/MyDrive/OLID'\n",
        "SAVE_PATH = '.drive/MyDrive/OLID/save'\n",
        "LABEL_DICT = {\n",
        "    'a': {'OFF': 0, 'NOT': 1},\n",
        "    'b': {'TIN': 0, 'UNT': 1, 'NULL': 2},\n",
        "    'c': {'IND': 0, 'GRP': 1, 'OTH': 2, 'NULL': 3}\n",
        "}\n",
        "\n",
        "TRAIN_PATH = os.path.join(OLID_PATH, 'olid-training-v1.0.tsv')"
      ],
      "metadata": {
        "id": "Gz2uPWG_2SWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = 'a'\n",
        "# model_name = 'roberta'\n",
        "# model_size = 'base'\n",
        "truncate = 512\n",
        "epochs = 10\n",
        "lr = 0.0001\n",
        "wd = 0.0\n",
        "bs = 50 #72\n",
        "patience = 5"
      ],
      "metadata": {
        "id": "-2mPqZlRE_I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "qBM-me1P0pfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def emoji2word(sents):\n",
        "    return [emoji.demojize(sent) for sent in sents]\n",
        "\n",
        "def remove_useless_punctuation(sents):\n",
        "    for i, sent in enumerate(sents):\n",
        "        sent = sent.replace(':', ' ')\n",
        "        sent = sent.replace('_', ' ')\n",
        "        sent = sent.replace('...', ' ')\n",
        "        sents[i] = sent\n",
        "    return sents\n",
        "\n",
        "def remove_replicates(sents):\n",
        "    # if there are multiple `@USER` tokens in a tweet, replace it with `@USERS`\n",
        "    # because some tweets contain so many `@USER` which may cause redundant\n",
        "    for i, sent in enumerate(sents):\n",
        "        if sent.find('@USER') != sent.rfind('@USER'):\n",
        "            sents[i] = sent.replace('@USER', '')\n",
        "            sents[i] = '@USERS ' + sents[i]\n",
        "    return sents\n",
        "\n",
        "def replace_rare_words(sents):\n",
        "    rare_words = {\n",
        "        'URL': 'http'\n",
        "    }\n",
        "    for i, sent in enumerate(sents):\n",
        "        for w in rare_words.keys():\n",
        "            sents[i] = sent.replace(w, rare_words[w])\n",
        "    return sents\n",
        "\n",
        "def segment_hashtag(sents):\n",
        "    # E.g. '#LunaticLeft' => 'lunatic left'\n",
        "    for i, sent in enumerate(sents):\n",
        "        sent_tokens = sent.split(' ')\n",
        "        for j, t in enumerate(sent_tokens):\n",
        "            if t.find('#') == 0:\n",
        "                sent_tokens[j] = ' '.join(segment(t))\n",
        "        sents[i] = ' '.join(sent_tokens)\n",
        "    return sents\n",
        "\n",
        "\n",
        "def process_tweets(tweets):\n",
        "    tweets = emoji2word(tweets)\n",
        "    tweets = replace_rare_words(tweets)\n",
        "    tweets = remove_replicates(tweets)\n",
        "    tweets = segment_hashtag(tweets)\n",
        "    tweets = remove_useless_punctuation(tweets)\n",
        "    tweets = np.array(tweets)\n",
        "    return tweets"
      ],
      "metadata": {
        "id": "teD7vqkUyTga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(TRAIN_PATH, sep='\\t', keep_default_na=False)\n",
        "\n",
        "if task == 'a':\n",
        "        df = df\n",
        "elif task == 'b':\n",
        "        df=df[~df['subtask_b'].isin(['NULL'])]\n",
        "elif task == 'c':\n",
        "        df=df[~df['subtask_c'].isin(['NULL'])]\n",
        "\n",
        "ids = np.array(df['id'].values)\n",
        "tweets = np.array(df['tweet'].values)\n",
        "\n",
        "print(tweets[1])\n",
        "# Process tweets\n",
        "df['tweet'] = process_tweets(tweets)\n",
        "\n",
        "tweets_2 = df['tweet']\n",
        "print(tweets_2[1])\n",
        "\n",
        "label_a = np.array(df['subtask_a'].values)\n",
        "label_b = df['subtask_b'].values\n",
        "label_c = np.array(df['subtask_c'].values)\n",
        "nums = len(df)\n",
        "\n",
        "if task == 'a':\n",
        "        df_train = df.copy()\n",
        "        df_train = df_train.drop(['subtask_b','subtask_c'],axis = 1)\n",
        "elif task == 'b':\n",
        "        df_train = df.copy()\n",
        "        df_train = df_train.drop(['subtask_a','subtask_c'],axis = 1)\n",
        "        df_train=df_train[~df_train['subtask_b'].isin(['NULL'])]\n",
        "elif task == 'c':\n",
        "        df_train = df.copy()\n",
        "        df_train = df_train.drop(['subtask_a','subtask_b'],axis = 1)\n",
        "        df_train=df_train[~df_train['subtask_c'].isin(['NULL'])]\n",
        "\n",
        "\n",
        "\n",
        "# read_test_file(task, tokenizer, truncate=512):\n",
        "df1 = pd.read_csv(os.path.join(OLID_PATH, 'testset-level' + task + '.tsv'), sep='\\t')\n",
        "df2 = pd.read_csv(os.path.join(OLID_PATH, 'labels-level' + task + '.csv'), sep=',')\n",
        "ids = np.array(df1['id'].values)\n",
        "# tweets = np.array(df1['tweet'].values)\n",
        "# labels = np.array(df2['label'].values)\n",
        "# nums = len(df1)\n",
        "df_test = pd.merge(df1,df2,on='id')\n",
        "# Process tweets\n",
        "tweets_test = df_test['tweet']\n",
        "df_test['tweet'] = process_tweets(tweets_test)\n",
        "\n",
        "#     # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "#     token_ids = [tokenizer(text=tweets[i], add_special_tokens=True, max_length=truncate) for i in range(nums)]\n",
        "#     mask = np.array(get_mask(token_ids))\n",
        "#     lens = get_lens(token_ids)\n",
        "#     token_ids = np.array(pad_sents(token_ids, tokenizer.pad_token_id))\n",
        "\n",
        "#     return ids, token_ids, lens, mask, labels"
      ],
      "metadata": {
        "id": "lv0Y41auy6Vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(emoji.demojize('Python is 👍'))"
      ],
      "metadata": {
        "id": "o8taarN3Nj69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tweets[:20])"
      ],
      "metadata": {
        "id": "-uci9DClL-oQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tweets_2)"
      ],
      "metadata": {
        "id": "YTYnYu7YL-k0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)"
      ],
      "metadata": {
        "id": "j2haVEqQJpUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_train)"
      ],
      "metadata": {
        "id": "zgm618fkJsT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_test)"
      ],
      "metadata": {
        "id": "QaQ5HnVDHBZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tokenizer (TF-IDF)"
      ],
      "metadata": {
        "id": "WDfdxtffy_lH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF feature extract\n",
        "# Create the numericalizer TFIDF for lowercase\n",
        "tfidf_vec  = TfidfVectorizer(decode_error='ignore', lowercase=True, stop_words=\"english\")\n",
        "\n",
        "# # translate the input text data\n",
        "# # Numericalize the train combined sentence\n",
        "X_train = tweets_2\n",
        "X_val = tweets_test\n",
        "Y_train_name = 'subtask_'+ task\n",
        "y_train = df_train[Y_train_name]\n",
        "y_val = df_test['label']\n",
        "x_train_tfidf_matrix = tfidf_vec.fit_transform(X_train.astype('U'))\n",
        "# Numericalize the test Headline\n",
        "x_valid_tfidf_matrix = tfidf_vec.transform(X_val.astype('U'))"
      ],
      "metadata": {
        "id": "zM7_sXeUy6Q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "OOcaPW8CNNqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_tfidf_matrix"
      ],
      "metadata": {
        "id": "MF1CfRyOLXPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Brief Visualization"
      ],
      "metadata": {
        "id": "D8z4eao4zOrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train size: ', x_train_tfidf_matrix.shape)\n",
        "print('Val size: ', x_valid_tfidf_matrix.shape)"
      ],
      "metadata": {
        "id": "pVio6kchzVwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = np.asarray(tfidf_vec.get_feature_names())\n",
        "print(dictionary[np.random.randint(0,len(dictionary),size=20)])"
      ],
      "metadata": {
        "id": "ZqtJMabnzQ5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['tweet']"
      ],
      "metadata": {
        "id": "7ow7cAUjcxjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "bLDDg4Bgy6wr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "tbnXRFdh7cLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC, SVC\n",
        "# load the machine model form sklearn\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import model_selection, naive_bayes, svm\n",
        "\n",
        "# Define the parameters to tune\n",
        "parameter_grid = { \n",
        "    'C': [1.0, 10],\n",
        "    'gamma': [1, 'auto', 'scale']\n",
        "}\n",
        "# # setup the SVM model\n",
        "SVM_model = GridSearchCV(svm.SVC(kernel='linear'), parameter_grid, cv=5, n_jobs=-1)\n",
        "# SVM_model = LinearSVC(C=1, loss=\"hinge\")\n",
        "# SVM_model = SVC(kernel=\"linear\")\n",
        "# fit the model with training dataset\n",
        "# SVM_model.fit(x_train_tfidf_matrix, y_train)\n",
        "\n",
        "SVM_model.fit(x_train_tfidf_matrix, y_train)"
      ],
      "metadata": {
        "id": "3Y5oaeiOy6On"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search = GridSearchCV(SVM_model, parameter_grid, cv=5,\n",
        "                           n_jobs =-1,\n",
        "                           scoring='accuracy',\n",
        "                           return_train_score=True)\n",
        "\n",
        "grid_search.fit(x_train_tfidf_matrix, y_train)\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "qRr2gnfEXOpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_SVM = SVM_model.predict(x_valid_tfidf_matrix)"
      ],
      "metadata": {
        "id": "XpoSpyQqlsq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(predictions_SVM)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iVNOsAxyul4",
        "outputId": "aa414b36-1cac-4e1e-f4bd-a3554e42e80f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "860"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# on test dataset\n",
        "cm = metrics.confusion_matrix(y_val, predictions_SVM)\n",
        "\n",
        "print('Confusion matrix：', cm, sep='\\n')\n",
        "\n",
        "TN = cm[0][0]\n",
        "FP = cm[0][1]\n",
        "FN = cm[1][0]\n",
        "TP = cm[1][1]\n",
        "\n",
        "# print(FP, FN, TP, TN)\n",
        "\n",
        "# accuracy: (TP + TN) / (TP + TN + FP + FN)\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "print(\"Accuracy: {:.3}%\".format(accuracy*100))\n",
        "\n",
        "# sensitivity(recall rate): TP / (TP + FN)\n",
        "sensitivity = TP / (TP + FN)\n",
        "print(\"Sensitivity: {:.3}%\".format(sensitivity*100))\n",
        "\n",
        "# specificity: TN / (FP + TN)\n",
        "specificity = TN / (FP + TN)\n",
        "print(\"Specificity: {:.3}%\".format(specificity*100))\n",
        "\n",
        "# precision: TP / (TP + FP)\n",
        "precision = TP / (TP + FP)\n",
        "print(\"Precision: {:.3}%\".format(precision*100))\n",
        "\n",
        "# F1 score: 2 * (precision * sensitivity) / (precision + sensitivity)\n",
        "print(\"F1 score: {:.3}%\".format(2 * (precision * sensitivity) / (precision + sensitivity)*100))\n",
        "\n",
        "# classification report\n",
        "r = metrics.classification_report(y_val, predictions_SVM)\n",
        "print('Classification report：', r, sep='\\n')"
      ],
      "metadata": {
        "id": "E-Ymn1IuOU4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest"
      ],
      "metadata": {
        "id": "edGNHLzXhe4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RF = RandomForestClassifier()\n",
        "RF.fit(x_train_tfidf_matrix, y_train)\n",
        "# show model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krt8cp8NhjA7",
        "outputId": "14827dbd-6e0d-4b15-a9ef-77b830022b5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier()"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the train score from the train set\n",
        "print(\"SVM Training Score -> {}%\".format(RF.score(x_train_tfidf_matrix, y_train)*100))\n",
        "# predict the result labels on validation dataset\n",
        "predictions_RF = RF.predict(x_valid_tfidf_matrix)\n",
        "# predict vith the valid data set\n",
        "print(\"SVM Accuracy Score -> {}%\".format(RF.score(x_valid_tfidf_matrix, y_val)*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frVf5uLph1T0",
        "outputId": "9efa1578-d7bb-4591-a5b9-1d34149487ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Training Score -> 99.89680082559339%\n",
            "SVM Accuracy Score -> 62.91079812206573%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# on test dataset\n",
        "cm = metrics.confusion_matrix(y_val, predictions_RF)\n",
        "\n",
        "print('Confusion matrix：', cm, sep='\\n')\n",
        "\n",
        "TN = cm[0][0]\n",
        "FP = cm[0][1]\n",
        "FN = cm[1][0]\n",
        "TP = cm[1][1]\n",
        "\n",
        "# print(FP, FN, TP, TN)\n",
        "\n",
        "# accuracy: (TP + TN) / (TP + TN + FP + FN)\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "print(\"Accuracy: {:.3}%\".format(accuracy*100))\n",
        "\n",
        "# sensitivity(recall rate): TP / (TP + FN)\n",
        "sensitivity = TP / (TP + FN)\n",
        "print(\"Sensitivity: {:.3}%\".format(sensitivity*100))\n",
        "\n",
        "# specificity: TN / (FP + TN)\n",
        "specificity = TN / (FP + TN)\n",
        "print(\"Specificity: {:.3}%\".format(specificity*100))\n",
        "\n",
        "# precision: TP / (TP + FP)\n",
        "precision = TP / (TP + FP)\n",
        "print(\"Precision: {:.3}%\".format(precision*100))\n",
        "\n",
        "# F1 score: 2 * (precision * sensitivity) / (precision + sensitivity)\n",
        "print(\"F1 score: {:.3}%\".format(2 * (precision * sensitivity) / (precision + sensitivity)*100))\n",
        "\n",
        "# classification report\n",
        "r = metrics.classification_report(y_val, predictions_RF)\n",
        "print('Classification report：', r, sep='\\n')"
      ],
      "metadata": {
        "id": "wuhQKkCah5Gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = [\n",
        "    {'n_estimators': [5, 20, 40, 60], 'max_features': [2, 12, 24, 48]},\n",
        "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
        "  ]\n",
        "\n",
        "grid_search = GridSearchCV(RF, param_grid, cv=5,\n",
        "                           scoring='accuracy',\n",
        "                           return_train_score=True)\n",
        "\n",
        "grid_search.fit(x_train_tfidf_matrix, y_train)"
      ],
      "metadata": {
        "id": "6lLv0spGTDcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BatujYbeTFbA",
        "outputId": "365dbd9b-85cc-4b3f-97db-82ac79c5d2cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'max_features': 28, 'n_estimators': 30}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RF_new = RandomForestClassifier(n_estimators = 30,max_features = 30).fit(x_train_tfidf_matrix, y_train)\n",
        "\n",
        "predictions_RF_new = RF_new.predict(x_valid_tfidf_matrix)\n",
        "# on test dataset\n",
        "cm = metrics.confusion_matrix(y_val, predictions_RF_new)\n",
        "\n",
        "print('Confusion matrix：', cm, sep='\\n')\n",
        "\n",
        "TN = cm[0][0]\n",
        "FP = cm[0][1]\n",
        "FN = cm[1][0]\n",
        "TP = cm[1][1]\n",
        "\n",
        "# print(FP, FN, TP, TN)\n",
        "\n",
        "# accuracy: (TP + TN) / (TP + TN + FP + FN)\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "print(\"Accuracy: {:.3}%\".format(accuracy*100))\n",
        "\n",
        "# sensitivity(recall rate): TP / (TP + FN)\n",
        "sensitivity = TP / (TP + FN)\n",
        "print(\"Sensitivity: {:.3}%\".format(sensitivity*100))\n",
        "\n",
        "# specificity: TN / (FP + TN)\n",
        "specificity = TN / (FP + TN)\n",
        "print(\"Specificity: {:.3}%\".format(specificity*100))\n",
        "\n",
        "# precision: TP / (TP + FP)\n",
        "precision = TP / (TP + FP)\n",
        "print(\"Precision: {:.3}%\".format(precision*100))\n",
        "\n",
        "# F1 score: 2 * (precision * sensitivity) / (precision + sensitivity)\n",
        "print(\"F1 score: {:.3}%\".format(2 * (precision * sensitivity) / (precision + sensitivity)*100))\n",
        "\n",
        "# classification report\n",
        "r = metrics.classification_report(y_val, predictions_RF_new)\n",
        "print('Classification report：', r, sep='\\n')"
      ],
      "metadata": {
        "id": "BXnnhfVnTKjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression"
      ],
      "metadata": {
        "id": "9_wonRCk8MDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LR = LogisticRegression()\n",
        "LR.fit(x_train_tfidf_matrix, y_train)\n",
        "# show model\n"
      ],
      "metadata": {
        "id": "V68cybSS8LlR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee3fdbb1-48ef-4f52-fe89-02d8b216ae76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the train score from the train set\n",
        "print(\"SVM Training Score -> {}%\".format(LR.score(x_train_tfidf_matrix, y_train)*100))\n",
        "# predict the result labels on validation dataset\n",
        "predictions_LR = LR.predict(x_valid_tfidf_matrix)\n",
        "# predict vith the valid data set\n",
        "print(\"SVM Accuracy Score -> {}%\".format(LR.score(x_valid_tfidf_matrix, y_val)*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFt9xZz3hJtq",
        "outputId": "5dde9ecd-7200-4b65-cf93-6ab67660cb7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Training Score -> 81.99174406604747%\n",
            "SVM Accuracy Score -> 64.7887323943662%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# on test dataset\n",
        "cm = metrics.confusion_matrix(y_val, predictions_LR)\n",
        "\n",
        "print('Confusion matrix：', cm, sep='\\n')\n",
        "\n",
        "TN = cm[0][0]\n",
        "FP = cm[0][1]\n",
        "FN = cm[1][0]\n",
        "TP = cm[1][1]\n",
        "\n",
        "# print(FP, FN, TP, TN)\n",
        "\n",
        "# accuracy: (TP + TN) / (TP + TN + FP + FN)\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "print(\"Accuracy: {:.3}%\".format(accuracy*100))\n",
        "\n",
        "# sensitivity(recall rate): TP / (TP + FN)\n",
        "sensitivity = TP / (TP + FN)\n",
        "print(\"Sensitivity: {:.3}%\".format(sensitivity*100))\n",
        "\n",
        "# specificity: TN / (FP + TN)\n",
        "specificity = TN / (FP + TN)\n",
        "print(\"Specificity: {:.3}%\".format(specificity*100))\n",
        "\n",
        "# precision: TP / (TP + FP)\n",
        "precision = TP / (TP + FP)\n",
        "print(\"Precision: {:.3}%\".format(precision*100))\n",
        "\n",
        "# F1 score: 2 * (precision * sensitivity) / (precision + sensitivity)\n",
        "print(\"F1 score: {:.3}%\".format(2 * (precision * sensitivity) / (precision + sensitivity)*100))\n",
        "\n",
        "# classification report\n",
        "r = metrics.classification_report(y_val, predictions_LR)\n",
        "print('Classification report：', r, sep='\\n')"
      ],
      "metadata": {
        "id": "czF2ZlQF8Liq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
        "clf = GridSearchCV(LogisticRegression(penalty='l2'), param_grid)\n",
        "GridSearchCV(cv=None,\n",
        "             estimator=LogisticRegression(C=1.0, intercept_scaling=1,   \n",
        "               dual=False, fit_intercept=True, penalty='l2', tol=0.0001),\n",
        "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]})"
      ],
      "metadata": {
        "id": "I7G6XR3g8LcL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bb1dc34-9a77-4bff-bce3-00a599923155"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(estimator=LogisticRegression(),\n",
              "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LRparam_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'max_iter': list(range(100,800,100)),\n",
        "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
        "}\n",
        "LR_search = GridSearchCV(LR, param_grid=LRparam_grid, refit = True, verbose = 3, cv=5)\n",
        "\n",
        "# fitting the model for grid search \n",
        "LR_search.fit(x_train_tfidf_matrix, y_train)\n",
        "LR_search.best_params_\n",
        "# summarize\n",
        "print('Mean Accuracy: %.3f' % LR_search.best_score_)\n",
        "print('Config: %s' % LR_search.best_params_)"
      ],
      "metadata": {
        "id": "sMpfYd5wOW4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR_search.best_params_"
      ],
      "metadata": {
        "id": "VPYFrzC_wkYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
      ],
      "metadata": {
        "id": "e262a_azTl50"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}