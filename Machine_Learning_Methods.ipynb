{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine_Learning_Methods.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMi8C6/byR7B+XiMIG/s72J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hongzf/OLID/blob/main/Machine_Learning_Methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCz94PP_yJ8z"
      },
      "outputs": [],
      "source": [
        "!pip install emoji\n",
        "!pip install wordsegment\n",
        "!pip install transformers\n",
        "!pip install trainer\n",
        "!pip install attention"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import os\n",
        "import emoji\n",
        "from wordsegment import load, segment\n",
        "import pickle\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import os,re\n",
        "from bs4 import BeautifulSoup\n",
        "from gensim import models\n",
        "\n",
        "from transformers import BertTokenizer, RobertaTokenizer, get_cosine_schedule_with_warmup\n",
        "from trainer import Trainer \n",
        "from torch import nn\n",
        "from transformers import BertModel, BertForSequenceClassification, RobertaForSequenceClassification, RobertaModel\n",
        "from attention import Attention\n",
        "load()"
      ],
      "metadata": {
        "id": "tq0b2L4T3yfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#optional Google drive integration\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjY4WW5w4AOy",
        "outputId": "0508e32d-689a-424b-cc77-d917bb0ff1b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OLID_PATH = './drive/MyDrive/OLID'\n",
        "SAVE_PATH = '.drive/MyDrive/OLID/save'\n",
        "LABEL_DICT = {\n",
        "    'a': {'OFF': 0, 'NOT': 1},\n",
        "    'b': {'TIN': 0, 'UNT': 1, 'NULL': 2},\n",
        "    'c': {'IND': 0, 'GRP': 1, 'OTH': 2, 'NULL': 3}\n",
        "}\n",
        "\n",
        "TRAIN_PATH = os.path.join(OLID_PATH, 'olid-training-v1.0.tsv')"
      ],
      "metadata": {
        "id": "Gz2uPWG_2SWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = 'a'\n",
        "# model_name = 'roberta'\n",
        "# model_size = 'base'\n",
        "truncate = 512\n",
        "epochs = 10\n",
        "lr = 0.0001\n",
        "wd = 0.0\n",
        "bs = 50 #72\n",
        "patience = 5"
      ],
      "metadata": {
        "id": "-2mPqZlRE_I6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "qBM-me1P0pfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def emoji2word(sents):\n",
        "    return [emoji.demojize(sent) for sent in sents]\n",
        "\n",
        "def remove_useless_punctuation(sents):\n",
        "    for i, sent in enumerate(sents):\n",
        "        sent = sent.replace(':', ' ')\n",
        "        sent = sent.replace('_', ' ')\n",
        "        sent = sent.replace('...', ' ')\n",
        "        sents[i] = sent\n",
        "    return sents\n",
        "\n",
        "def remove_replicates(sents):\n",
        "    # if there are multiple `@USER` tokens in a tweet, replace it with `@USERS`\n",
        "    # because some tweets contain so many `@USER` which may cause redundant\n",
        "    for i, sent in enumerate(sents):\n",
        "        if sent.find('@USER') != sent.rfind('@USER'):\n",
        "            sents[i] = sent.replace('@USER', '')\n",
        "            sents[i] = '@USERS ' + sents[i]\n",
        "    return sents\n",
        "\n",
        "def replace_rare_words(sents):\n",
        "    rare_words = {\n",
        "        'URL': 'http'\n",
        "    }\n",
        "    for i, sent in enumerate(sents):\n",
        "        for w in rare_words.keys():\n",
        "            sents[i] = sent.replace(w, rare_words[w])\n",
        "    return sents\n",
        "\n",
        "def segment_hashtag(sents):\n",
        "    # E.g. '#LunaticLeft' => 'lunatic left'\n",
        "    for i, sent in enumerate(sents):\n",
        "        sent_tokens = sent.split(' ')\n",
        "        for j, t in enumerate(sent_tokens):\n",
        "            if t.find('#') == 0:\n",
        "                sent_tokens[j] = ' '.join(segment(t))\n",
        "        sents[i] = ' '.join(sent_tokens)\n",
        "    return sents\n",
        "\n",
        "\n",
        "def process_tweets(tweets):\n",
        "    tweets = emoji2word(tweets)\n",
        "    tweets = replace_rare_words(tweets)\n",
        "    tweets = remove_replicates(tweets)\n",
        "    tweets = segment_hashtag(tweets)\n",
        "    tweets = remove_useless_punctuation(tweets)\n",
        "    tweets = np.array(tweets)\n",
        "    return tweets"
      ],
      "metadata": {
        "id": "teD7vqkUyTga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(TRAIN_PATH, sep='\\t', keep_default_na=False)\n",
        "\n",
        "if task == 'a':\n",
        "        df = df\n",
        "elif task == 'b':\n",
        "        df=df[~df['subtask_b'].isin(['NULL'])]\n",
        "elif task == 'c':\n",
        "        df=df[~df['subtask_c'].isin(['NULL'])]\n",
        "\n",
        "ids = np.array(df['id'].values)\n",
        "tweets = np.array(df['tweet'].values)\n",
        "\n",
        "print(tweets[1])\n",
        "# Process tweets\n",
        "df['tweet'] = process_tweets(tweets)\n",
        "\n",
        "tweets_2 = df['tweet']\n",
        "print(tweets_2[1])\n",
        "\n",
        "label_a = np.array(df['subtask_a'].values)\n",
        "label_b = df['subtask_b'].values\n",
        "label_c = np.array(df['subtask_c'].values)\n",
        "nums = len(df)\n",
        "\n",
        "if task == 'a':\n",
        "        df_train = df.copy()\n",
        "        df_train = df_train.drop(['subtask_b','subtask_c'],axis = 1)\n",
        "elif task == 'b':\n",
        "        df_train = df.copy()\n",
        "        df_train = df_train.drop(['subtask_a','subtask_c'],axis = 1)\n",
        "        df_train=df_train[~df_train['subtask_b'].isin(['NULL'])]\n",
        "elif task == 'c':\n",
        "        df_train = df.copy()\n",
        "        df_train = df_train.drop(['subtask_a','subtask_b'],axis = 1)\n",
        "        df_train=df_train[~df_train['subtask_c'].isin(['NULL'])]\n",
        "\n",
        "\n",
        "\n",
        "# read_test_file(task, tokenizer, truncate=512):\n",
        "df1 = pd.read_csv(os.path.join(OLID_PATH, 'testset-level' + task + '.tsv'), sep='\\t')\n",
        "df2 = pd.read_csv(os.path.join(OLID_PATH, 'labels-level' + task + '.csv'), sep=',')\n",
        "ids = np.array(df1['id'].values)\n",
        "# tweets = np.array(df1['tweet'].values)\n",
        "# labels = np.array(df2['label'].values)\n",
        "# nums = len(df1)\n",
        "df_test = pd.merge(df1,df2,on='id')\n",
        "# Process tweets\n",
        "tweets_test = df_test['tweet']\n",
        "df_test['tweet'] = process_tweets(tweets_test)\n",
        "\n",
        "#     # tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "#     token_ids = [tokenizer(text=tweets[i], add_special_tokens=True, max_length=truncate) for i in range(nums)]\n",
        "#     mask = np.array(get_mask(token_ids))\n",
        "#     lens = get_lens(token_ids)\n",
        "#     token_ids = np.array(pad_sents(token_ids, tokenizer.pad_token_id))\n",
        "\n",
        "#     return ids, token_ids, lens, mask, labels"
      ],
      "metadata": {
        "id": "lv0Y41auy6Vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(emoji.demojize('Python is ðŸ‘'))"
      ],
      "metadata": {
        "id": "o8taarN3Nj69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tweets[:20])"
      ],
      "metadata": {
        "id": "-uci9DClL-oQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tweets_2)"
      ],
      "metadata": {
        "id": "YTYnYu7YL-k0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df)"
      ],
      "metadata": {
        "id": "j2haVEqQJpUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_train)"
      ],
      "metadata": {
        "id": "zgm618fkJsT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_test)"
      ],
      "metadata": {
        "id": "QaQ5HnVDHBZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tokenizer (TF-IDF)"
      ],
      "metadata": {
        "id": "WDfdxtffy_lH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF feature extract\n",
        "# Create the numericalizer TFIDF for lowercase\n",
        "tfidf_vec  = TfidfVectorizer(decode_error='ignore', lowercase=True, stop_words=\"english\")\n",
        "\n",
        "# # translate the input text data\n",
        "# # Numericalize the train combined sentence\n",
        "X_train = tweets_2\n",
        "X_val = tweets_test\n",
        "Y_train_name = 'subtask_'+ task\n",
        "y_train = df_train[Y_train_name]\n",
        "y_val = df_test['label']\n",
        "x_train_tfidf_matrix = tfidf_vec.fit_transform(X_train.astype('U'))\n",
        "# Numericalize the test Headline\n",
        "x_valid_tfidf_matrix = tfidf_vec.transform(X_val.astype('U'))"
      ],
      "metadata": {
        "id": "zM7_sXeUy6Q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "id": "OOcaPW8CNNqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_tfidf_matrix"
      ],
      "metadata": {
        "id": "MF1CfRyOLXPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Brief Visualization"
      ],
      "metadata": {
        "id": "D8z4eao4zOrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train size: ', x_train_tfidf_matrix.shape)\n",
        "print('Val size: ', x_valid_tfidf_matrix.shape)"
      ],
      "metadata": {
        "id": "pVio6kchzVwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = np.asarray(tfidf_vec.get_feature_names())\n",
        "print(dictionary[np.random.randint(0,len(dictionary),size=20)])"
      ],
      "metadata": {
        "id": "ZqtJMabnzQ5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['tweet']"
      ],
      "metadata": {
        "id": "7ow7cAUjcxjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model"
      ],
      "metadata": {
        "id": "bLDDg4Bgy6wr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "tbnXRFdh7cLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC, SVC\n",
        "# load the machine model form sklearn\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import model_selection, naive_bayes, svm\n",
        "\n",
        "# Define the parameters to tune\n",
        "parameter_grid = { \n",
        "    'C': [1.0, 10],\n",
        "    'gamma': [1, 'auto', 'scale']\n",
        "}\n",
        "# # setup the SVM model\n",
        "SVM_model = GridSearchCV(svm.SVC(kernel='linear'), parameter_grid, cv=5, n_jobs=-1)\n",
        "# SVM_model = LinearSVC(C=1, loss=\"hinge\")\n",
        "# SVM_model = SVC(kernel=\"linear\")\n",
        "# fit the model with training dataset\n",
        "# SVM_model.fit(x_train_tfidf_matrix, y_train)\n",
        "\n",
        "SVM_model.fit(x_train_tfidf_matrix, y_train)"
      ],
      "metadata": {
        "id": "3Y5oaeiOy6On"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search = GridSearchCV(SVM_model, parameter_grid, cv=5,\n",
        "                           n_jobs =-1,\n",
        "                           scoring='accuracy',\n",
        "                           return_train_score=True)\n",
        "\n",
        "grid_search.fit(x_train_tfidf_matrix, y_train)\n",
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "id": "qRr2gnfEXOpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_SVM = SVM_model.predict(x_valid_tfidf_matrix)"
      ],
      "metadata": {
        "id": "XpoSpyQqlsq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(predictions_SVM)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iVNOsAxyul4",
        "outputId": "aa414b36-1cac-4e1e-f4bd-a3554e42e80f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "860"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# on test dataset\n",
        "cm = metrics.confusion_matrix(y_val, predictions_SVM)\n",
        "\n",
        "print('Confusion matrixï¼š', cm, sep='\\n')\n",
        "\n",
        "TN = cm[0][0]\n",
        "FP = cm[0][1]\n",
        "FN = cm[1][0]\n",
        "TP = cm[1][1]\n",
        "\n",
        "# print(FP, FN, TP, TN)\n",
        "\n",
        "# accuracy: (TP + TN) / (TP + TN + FP + FN)\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "print(\"Accuracy: {:.3}%\".format(accuracy*100))\n",
        "\n",
        "# sensitivity(recall rate): TP / (TP + FN)\n",
        "sensitivity = TP / (TP + FN)\n",
        "print(\"Sensitivity: {:.3}%\".format(sensitivity*100))\n",
        "\n",
        "# specificity: TN / (FP + TN)\n",
        "specificity = TN / (FP + TN)\n",
        "print(\"Specificity: {:.3}%\".format(specificity*100))\n",
        "\n",
        "# precision: TP / (TP + FP)\n",
        "precision = TP / (TP + FP)\n",
        "print(\"Precision: {:.3}%\".format(precision*100))\n",
        "\n",
        "# F1 score: 2 * (precision * sensitivity) / (precision + sensitivity)\n",
        "print(\"F1 score: {:.3}%\".format(2 * (precision * sensitivity) / (precision + sensitivity)*100))\n",
        "\n",
        "# classification report\n",
        "r = metrics.classification_report(y_val, predictions_SVM)\n",
        "print('Classification reportï¼š', r, sep='\\n')"
      ],
      "metadata": {
        "id": "E-Ymn1IuOU4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest"
      ],
      "metadata": {
        "id": "edGNHLzXhe4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RF = RandomForestClassifier()\n",
        "RF.fit(x_train_tfidf_matrix, y_train)\n",
        "# show model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krt8cp8NhjA7",
        "outputId": "14827dbd-6e0d-4b15-a9ef-77b830022b5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier()"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the train score from the train set\n",
        "print(\"SVM Training Score -> {}%\".format(RF.score(x_train_tfidf_matrix, y_train)*100))\n",
        "# predict the result labels on validation dataset\n",
        "predictions_RF = RF.predict(x_valid_tfidf_matrix)\n",
        "# predict vith the valid data set\n",
        "print(\"SVM Accuracy Score -> {}%\".format(RF.score(x_valid_tfidf_matrix, y_val)*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frVf5uLph1T0",
        "outputId": "9efa1578-d7bb-4591-a5b9-1d34149487ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Training Score -> 99.89680082559339%\n",
            "SVM Accuracy Score -> 62.91079812206573%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# on test dataset\n",
        "cm = metrics.confusion_matrix(y_val, predictions_RF)\n",
        "\n",
        "print('Confusion matrixï¼š', cm, sep='\\n')\n",
        "\n",
        "TN = cm[0][0]\n",
        "FP = cm[0][1]\n",
        "FN = cm[1][0]\n",
        "TP = cm[1][1]\n",
        "\n",
        "# print(FP, FN, TP, TN)\n",
        "\n",
        "# accuracy: (TP + TN) / (TP + TN + FP + FN)\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "print(\"Accuracy: {:.3}%\".format(accuracy*100))\n",
        "\n",
        "# sensitivity(recall rate): TP / (TP + FN)\n",
        "sensitivity = TP / (TP + FN)\n",
        "print(\"Sensitivity: {:.3}%\".format(sensitivity*100))\n",
        "\n",
        "# specificity: TN / (FP + TN)\n",
        "specificity = TN / (FP + TN)\n",
        "print(\"Specificity: {:.3}%\".format(specificity*100))\n",
        "\n",
        "# precision: TP / (TP + FP)\n",
        "precision = TP / (TP + FP)\n",
        "print(\"Precision: {:.3}%\".format(precision*100))\n",
        "\n",
        "# F1 score: 2 * (precision * sensitivity) / (precision + sensitivity)\n",
        "print(\"F1 score: {:.3}%\".format(2 * (precision * sensitivity) / (precision + sensitivity)*100))\n",
        "\n",
        "# classification report\n",
        "r = metrics.classification_report(y_val, predictions_RF)\n",
        "print('Classification reportï¼š', r, sep='\\n')"
      ],
      "metadata": {
        "id": "wuhQKkCah5Gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = [\n",
        "    {'n_estimators': [5, 20, 40, 60], 'max_features': [2, 12, 24, 48]},\n",
        "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
        "  ]\n",
        "\n",
        "grid_search = GridSearchCV(RF, param_grid, cv=5,\n",
        "                           scoring='accuracy',\n",
        "                           return_train_score=True)\n",
        "\n",
        "grid_search.fit(x_train_tfidf_matrix, y_train)"
      ],
      "metadata": {
        "id": "6lLv0spGTDcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(grid_search.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BatujYbeTFbA",
        "outputId": "365dbd9b-85cc-4b3f-97db-82ac79c5d2cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'max_features': 28, 'n_estimators': 30}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RF_new = RandomForestClassifier(n_estimators = 30,max_features = 30).fit(x_train_tfidf_matrix, y_train)\n",
        "\n",
        "predictions_RF_new = RF_new.predict(x_valid_tfidf_matrix)\n",
        "# on test dataset\n",
        "cm = metrics.confusion_matrix(y_val, predictions_RF_new)\n",
        "\n",
        "print('Confusion matrixï¼š', cm, sep='\\n')\n",
        "\n",
        "TN = cm[0][0]\n",
        "FP = cm[0][1]\n",
        "FN = cm[1][0]\n",
        "TP = cm[1][1]\n",
        "\n",
        "# print(FP, FN, TP, TN)\n",
        "\n",
        "# accuracy: (TP + TN) / (TP + TN + FP + FN)\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "print(\"Accuracy: {:.3}%\".format(accuracy*100))\n",
        "\n",
        "# sensitivity(recall rate): TP / (TP + FN)\n",
        "sensitivity = TP / (TP + FN)\n",
        "print(\"Sensitivity: {:.3}%\".format(sensitivity*100))\n",
        "\n",
        "# specificity: TN / (FP + TN)\n",
        "specificity = TN / (FP + TN)\n",
        "print(\"Specificity: {:.3}%\".format(specificity*100))\n",
        "\n",
        "# precision: TP / (TP + FP)\n",
        "precision = TP / (TP + FP)\n",
        "print(\"Precision: {:.3}%\".format(precision*100))\n",
        "\n",
        "# F1 score: 2 * (precision * sensitivity) / (precision + sensitivity)\n",
        "print(\"F1 score: {:.3}%\".format(2 * (precision * sensitivity) / (precision + sensitivity)*100))\n",
        "\n",
        "# classification report\n",
        "r = metrics.classification_report(y_val, predictions_RF_new)\n",
        "print('Classification reportï¼š', r, sep='\\n')"
      ],
      "metadata": {
        "id": "BXnnhfVnTKjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression"
      ],
      "metadata": {
        "id": "9_wonRCk8MDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LR = LogisticRegression()\n",
        "LR.fit(x_train_tfidf_matrix, y_train)\n",
        "# show model\n"
      ],
      "metadata": {
        "id": "V68cybSS8LlR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee3fdbb1-48ef-4f52-fe89-02d8b216ae76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the train score from the train set\n",
        "print(\"SVM Training Score -> {}%\".format(LR.score(x_train_tfidf_matrix, y_train)*100))\n",
        "# predict the result labels on validation dataset\n",
        "predictions_LR = LR.predict(x_valid_tfidf_matrix)\n",
        "# predict vith the valid data set\n",
        "print(\"SVM Accuracy Score -> {}%\".format(LR.score(x_valid_tfidf_matrix, y_val)*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFt9xZz3hJtq",
        "outputId": "5dde9ecd-7200-4b65-cf93-6ab67660cb7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Training Score -> 81.99174406604747%\n",
            "SVM Accuracy Score -> 64.7887323943662%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# on test dataset\n",
        "cm = metrics.confusion_matrix(y_val, predictions_LR)\n",
        "\n",
        "print('Confusion matrixï¼š', cm, sep='\\n')\n",
        "\n",
        "TN = cm[0][0]\n",
        "FP = cm[0][1]\n",
        "FN = cm[1][0]\n",
        "TP = cm[1][1]\n",
        "\n",
        "# print(FP, FN, TP, TN)\n",
        "\n",
        "# accuracy: (TP + TN) / (TP + TN + FP + FN)\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "print(\"Accuracy: {:.3}%\".format(accuracy*100))\n",
        "\n",
        "# sensitivity(recall rate): TP / (TP + FN)\n",
        "sensitivity = TP / (TP + FN)\n",
        "print(\"Sensitivity: {:.3}%\".format(sensitivity*100))\n",
        "\n",
        "# specificity: TN / (FP + TN)\n",
        "specificity = TN / (FP + TN)\n",
        "print(\"Specificity: {:.3}%\".format(specificity*100))\n",
        "\n",
        "# precision: TP / (TP + FP)\n",
        "precision = TP / (TP + FP)\n",
        "print(\"Precision: {:.3}%\".format(precision*100))\n",
        "\n",
        "# F1 score: 2 * (precision * sensitivity) / (precision + sensitivity)\n",
        "print(\"F1 score: {:.3}%\".format(2 * (precision * sensitivity) / (precision + sensitivity)*100))\n",
        "\n",
        "# classification report\n",
        "r = metrics.classification_report(y_val, predictions_LR)\n",
        "print('Classification reportï¼š', r, sep='\\n')"
      ],
      "metadata": {
        "id": "czF2ZlQF8Liq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
        "clf = GridSearchCV(LogisticRegression(penalty='l2'), param_grid)\n",
        "GridSearchCV(cv=None,\n",
        "             estimator=LogisticRegression(C=1.0, intercept_scaling=1,   \n",
        "               dual=False, fit_intercept=True, penalty='l2', tol=0.0001),\n",
        "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]})"
      ],
      "metadata": {
        "id": "I7G6XR3g8LcL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bb1dc34-9a77-4bff-bce3-00a599923155"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(estimator=LogisticRegression(),\n",
              "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LRparam_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'max_iter': list(range(100,800,100)),\n",
        "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
        "}\n",
        "LR_search = GridSearchCV(LR, param_grid=LRparam_grid, refit = True, verbose = 3, cv=5)\n",
        "\n",
        "# fitting the model for grid search \n",
        "LR_search.fit(x_train_tfidf_matrix, y_train)\n",
        "LR_search.best_params_\n",
        "# summarize\n",
        "print('Mean Accuracy: %.3f' % LR_search.best_score_)\n",
        "print('Config: %s' % LR_search.best_params_)"
      ],
      "metadata": {
        "id": "sMpfYd5wOW4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR_search.best_params_"
      ],
      "metadata": {
        "id": "VPYFrzC_wkYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
      ],
      "metadata": {
        "id": "e262a_azTl50"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}